{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we declare the Environment we should shape our Message Object model and environment config model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Each Message Object contains the following attributes:\n",
    "    size: size of the message\n",
    "    source: source task [Task where the message is generated]\n",
    "    destination: destination task [Task where the message is meant to be sent]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MessageObject:\n",
    "    def __init__(self, size, source, destination):\n",
    "        self.size = size\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "\n",
    "    def to_array(self):\n",
    "        return [self.size, self.source, self.destination]\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Contains name and scaling factors of all the reward functions\n",
    "  Negative rewards:\n",
    "    message_pool_invalid_index_reward: when the message pool is accessed with an invalid index\n",
    "    comm_bus_invalid_index_reward: when the communication bus is accessed with an invalid index\n",
    "    comm_bus_overflow_reward: when the communication bus is overflowed\n",
    "  Positive rewards:\n",
    "    message_store_reward: when a message is stored in the message pool\n",
    "    message_send_reward: when a message is sent from the message pool to the communication bus\n",
    "    message_pool_cost_decrease_reward: when the message pool cost decreases\n",
    "    comm_bus_cost_decrease_reward: when the communication bus cost decreases\n",
    "'''\n",
    "class RewardConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_pool_invalid_index=-1,\n",
    "        comm_bus_invalid_index=-1,\n",
    "        comm_bus_overflow=-1,\n",
    "        message_store=1,\n",
    "        message_send=1,\n",
    "        message_pool_cost_decrease=1,\n",
    "        comm_bus_cost_decrease=1,\n",
    "    ):\n",
    "        # Negative rewards\n",
    "        self.message_pool_invalid_index_reward = message_pool_invalid_index\n",
    "        self.comm_bus_invalid_index_reward = comm_bus_invalid_index\n",
    "        self.comm_bus_overflow_reward = comm_bus_overflow\n",
    "\n",
    "        # Positive rewards\n",
    "        self.message_store_reward = message_store\n",
    "        self.message_send_reward = message_send\n",
    "        self.message_pool_cost_decrease_reward = message_pool_cost_decrease\n",
    "        self.comm_bus_cost_decrease_reward = comm_bus_cost_decrease\n",
    "\n",
    "    def load_from_file(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            self.message_pool_invalid_index_reward = data.get(\n",
    "                \"message_pool_invalid_index_reward\",\n",
    "                self.message_pool_invalid_index_reward,\n",
    "            )\n",
    "            self.comm_bus_invalid_index_reward = data.get(\n",
    "                \"comm_bus_invalid_index_reward\", self.comm_bus_invalid_index_reward\n",
    "            )\n",
    "            self.comm_bus_overflow_reward = data.get(\n",
    "                \"comm_bus_overflow_reward\", self.comm_bus_overflow_reward\n",
    "            )\n",
    "            self.message_store_reward = data.get(\n",
    "                \"message_store_reward\", self.message_store_reward\n",
    "            )\n",
    "            self.message_send_reward = data.get(\n",
    "                \"message_send_reward\", self.message_send_reward\n",
    "            )\n",
    "            self.message_pool_cost_decrease_reward = data.get(\n",
    "                \"message_pool_cost_decrease_reward\",\n",
    "                self.message_pool_cost_decrease_reward,\n",
    "            )\n",
    "            self.comm_bus_cost_decrease_reward = data.get(\n",
    "                \"comm_bus_cost_decrease_reward\", self.comm_bus_cost_decrease_reward\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "Environment Configuration\n",
    "    min_message_pool: minimum number of messages in the message pool\n",
    "    max_message_pool: maximum number of messages in the message pool\n",
    "    min_message_obj_size: minimum size of the message object\n",
    "    max_message_obj_size: maximum size of the message object\n",
    "    comm_bus_size: size of the communication bus\n",
    "\n",
    "    Can directly load from a json file or manual user input\n",
    "    Can be used to generate a random environment configuration\n",
    "'''\n",
    "class EnvConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_message_pool=10,\n",
    "        max_message_pool=25,\n",
    "        min_message_obj_size=50,\n",
    "        max_message_obj_size=300,\n",
    "        comm_bus_size=1500,\n",
    "    ):\n",
    "        self.min_message_pool = min_message_pool\n",
    "        self.max_message_pool = max_message_pool\n",
    "        self.min_message_obj_size = min_message_obj_size\n",
    "        self.max_message_obj_size = max_message_obj_size\n",
    "        self.comm_bus_size = comm_bus_size\n",
    "\n",
    "    def load_from_file(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            self.min_message_pool = data.get(\"min_message_pool\", self.min_message_pool)\n",
    "            self.max_message_pool = data.get(\"max_message_pool\", self.max_message_pool)\n",
    "            self.min_message_obj_size = data.get(\n",
    "                \"min_message_obj_size\", self.min_message_obj_size\n",
    "            )\n",
    "            self.max_message_obj_size = data.get(\n",
    "                \"max_message_obj_size\", self.max_message_obj_size\n",
    "            )\n",
    "            self.comm_bus_size = data.get(\"comm_bus_size\", self.comm_bus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "The Message Network Environment for CADES-Task 2 (Message Passing)\n",
    "    Observation Space:\n",
    "        3 - as each message is represented by 3 values (size, source, destination)\n",
    "        self.config.max_message_pool + self.config.comm_bus_size - as the message pool\n",
    "        and communication bus are both in observation space\n",
    "    Action Space:\n",
    "        [OPERATION, MESSAGE_POOL_INDEX, COMM_BUS_INDEX]\n",
    "        OPERATION: 0 - Indicates the node manager/Agent pick message from the message pool and queues in the bus. ,\n",
    "                   1 - Indicates the node manager/Agent pick message from the communication bus and sends it to destination node.\n",
    "        MESSAGE_POOL_INDEX: Index of the message object in the 'message pool' to be picked for sending \n",
    "        COMM_BUS_INDEX: Index of the message object in the 'communication bus' to be picked for sending to destination  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MessageNetworkEnv(gym.Env):\n",
    "    def __init__(self, envconfig, reward_config):\n",
    "        super(MessageNetworkEnv, self).__init__()\n",
    "\n",
    "        self.config = envconfig\n",
    "        self.reward_config = reward_config\n",
    "\n",
    "        # Observation Space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(3, self.config.max_message_pool + self.config.comm_bus_size),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Action Space\n",
    "        self.action_space = spaces.MultiDiscrete(\n",
    "            [2, self.config.max_message_pool, self.config.max_message_pool]\n",
    "        )\n",
    "\n",
    "        self.message_pool = []\n",
    "        self.comm_bus = []\n",
    "\n",
    "    def reset(self):\n",
    "        num_messages = np.random.randint(\n",
    "            self.config.min_message_pool, self.config.max_message_pool + 1\n",
    "        )\n",
    "        self.message_pool = [self.generate_message() for _ in range(num_messages)]\n",
    "        self.comm_bus = []\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    # Generates Message Objects\n",
    "    # Source and Destination tasks are now assigned at random, but we will have to change this later\n",
    "    def generate_message(self):\n",
    "        size = np.random.uniform(\n",
    "            self.config.min_message_obj_size / self.config.comm_bus_size,\n",
    "            self.config.max_message_obj_size / self.config.comm_bus_size,\n",
    "        )\n",
    "        source = np.random.randint(0, 100)\n",
    "        destination = np.random.randint(0, 100)\n",
    "\n",
    "        while (\n",
    "            destination == source\n",
    "        ):  # To ensure Source and destination are never the same\n",
    "            destination = np.random.randint(0, 100)\n",
    "\n",
    "        return MessageObject(size, source, destination)\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are used to convert the message pool and communication bus into an observation\n",
    "    \"\"\"\n",
    "\n",
    "    def get_message_pool_observation(self):\n",
    "        message_pool_observation = np.zeros(self.config.max_message_pool)\n",
    "        for i, message in enumerate(self.message_pool):\n",
    "            message_pool_observation[i] = message.size\n",
    "        return message_pool_observation\n",
    "\n",
    "    def get_comm_bus_observation(self):\n",
    "        comm_bus_observation = np.zeros(self.config.comm_bus_size)\n",
    "\n",
    "        # We are simply checking each message that exists within the communication bus\n",
    "        for i, message in enumerate(self.comm_bus):\n",
    "            comm_bus_observation[i] = message.size\n",
    "        return comm_bus_observation\n",
    "\n",
    "    def get_observation(self):\n",
    "        message_pool_observation = self.get_message_pool_observation()\n",
    "        comm_bus_observation = self.get_comm_bus_observation()\n",
    "\n",
    "        return np.array([message_pool_observation, comm_bus_observation])\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are used to convert the action into a message object\n",
    "    \"\"\"\n",
    "\n",
    "    def get_message_from_message_pool(self, message_pool_index):\n",
    "        return self.message_pool[message_pool_index]\n",
    "\n",
    "    def get_message_from_comm_bus(self, comm_bus_index):\n",
    "        return self.comm_bus[comm_bus_index]\n",
    "\n",
    "    def get_message_from_action(self, action):\n",
    "        operation, message_pool_index, comm_bus_index = action\n",
    "        if operation == 0:\n",
    "            return self.get_message_from_message_pool(message_pool_index)\n",
    "        elif operation == 1:\n",
    "            return self.get_message_from_comm_bus(comm_bus_index)\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are used to generate appropriate negative rewards based on observations\n",
    "        - MESSAGE_POOL_INVALID_INDEX_REWARD if it chooses an index that is not in the message pool.\n",
    "        - COMM_BUS_INVALID_INDEX_REWARD if it chooses an index that is not in the communication bus.\n",
    "\n",
    "        - COMM_BUS_OVERFLOW_REWARD if it tries to place a message object in a occupied communication bus \n",
    "        that makes it surpass its capacity.\n",
    "        \n",
    "        - MESSAGE_POOL_NO_CHANGE_REWARD if OPERATION=1 AND message objects are in the message pool \n",
    "        AND adequate space is available in the communication bus \n",
    "\n",
    "        - COMM_BUS_NO_CHANGE_REWARD if OPERATION=0 AND message objects in the communication bus. \n",
    "\n",
    "        ==== The below rewards are to be done later =====\n",
    "        -? MESSAGE_POOL_INVALID_INDEX_REWARD if it chooses a previously picked index from the message pool. \n",
    "        -? COMM_BUS_INVALID_INDEX_REWARD if it chooses a previously picked index from the communication bus.\n",
    "        - SENDER_TASK_NOT_ALLOCATED_REWARD if the sender task is not allocated to the node manager/Agent.\n",
    "        - RECEIVER_TASK_NOT_ALLOCATED_REWARD if the receiver task is not allocated to the node manager/Agent. \n",
    "    \"\"\"\n",
    "\n",
    "    def check_message_pool_invalid_index(self, message_pool_index):\n",
    "        if message_pool_index >= len(self.message_pool):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_comm_bus_invalid_index(self, comm_bus_index):\n",
    "        if comm_bus_index >= len(self.comm_bus):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_comm_bus_overflow(self, message):\n",
    "        if (\n",
    "            sum(message.size for message in self.comm_bus) + message.size\n",
    "            > self.config.comm_bus_size\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_message_pool_no_change(self, message):\n",
    "        if message in self.message_pool:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_comm_bus_no_change(self, message):\n",
    "        if message in self.comm_bus:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are used to generate appropriate positive rewards based on observations\n",
    "        - MESSAGE_STORE_REWARD if it chooses OPERATION=0 AND message objects are in the message pool\n",
    "        - MESSAGE_SEND_REWARD if it chooses OPERATION=1 AND message objects are in the communication bus\n",
    "        - MESSAGE_POOL_COST_DECREASE_REWARD if it chooses OPERATION=0 AND messages are in the message pool \n",
    "        AND the size of MESSAGE_POOL decreases. [Reward assigned based on the size of the message chosen]\n",
    "        - COMM_BUS_COST_DECREASE_REWARD if it chooses OPERATION=1 AND messages are in the communication bus\n",
    "        AND the size of COMM_BUS decreases. [Reward assigned based on the size of the message chosen]\n",
    "\n",
    "\n",
    "        ==== The below rewards are to be done later =====\n",
    "        -? MESSAGE_TRAVEL_DISTANCE_REWARD if OPERATION=1 AND ...\n",
    "    \"\"\"\n",
    "\n",
    "    def check_message_store_reward(self, message):\n",
    "        if message in self.message_pool:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_message_send_reward(self, message):\n",
    "        if message in self.comm_bus:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_message_pool_cost_decrease_reward(self, message):\n",
    "        message_size_range = (\n",
    "            self.config.max_message_obj_size - self.config.min_message_obj_size\n",
    "        )\n",
    "        normalized_size = (\n",
    "            message.size * self.config.comm_bus_size - self.config.min_message_obj_size\n",
    "        ) / message_size_range\n",
    "        reward = normalized_size * self.reward_config.message_pool_cost_decrease_reward\n",
    "        return reward\n",
    "\n",
    "    def get_comm_bus_cost_decrease_reward(self, message):\n",
    "        message_size_range = (\n",
    "            self.config.max_message_obj_size - self.config.min_message_obj_size\n",
    "        )\n",
    "        normalized_size = (\n",
    "            message.size * self.config.comm_bus_size - self.config.min_message_obj_size\n",
    "        ) / message_size_range\n",
    "        reward = normalized_size * self.reward_config.comm_bus_cost_decrease_reward\n",
    "        return reward\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are for the step function\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        operation, message_pool_index, comm_bus_index = action\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        if operation == 0:\n",
    "            message = self.get_message_from_action(action)\n",
    "            if self.check_message_pool_invalid_index(message_pool_index):\n",
    "                reward += self.reward_config.message_pool_invalid_index_reward\n",
    "            elif self.check_comm_bus_overflow(message):\n",
    "                reward += self.reward_config.comm_bus_overflow_reward\n",
    "            elif self.check_message_pool_no_change(message):\n",
    "                reward += self.reward_config.message_pool_no_change_reward\n",
    "            else:\n",
    "                self.message_pool.remove(message)\n",
    "                self.comm_bus.append(message)\n",
    "                reward = (\n",
    "                    self.reward_config.message_store_reward\n",
    "                    + self.get_message_pool_cost_decrease_reward(message)\n",
    "                )\n",
    "        elif operation == 1:\n",
    "            message = self.get_message_from_action(action)\n",
    "            if self.check_comm_bus_invalid_index(comm_bus_index):\n",
    "                reward += self.reward_config.comm_bus_invalid_index_reward\n",
    "            elif self.check_comm_bus_no_change(message):\n",
    "                reward += self.reward_config.comm_bus_no_change_reward\n",
    "            else:\n",
    "                self.comm_bus.remove(message)\n",
    "                reward = (\n",
    "                    self.reward_config.message_send_reward\n",
    "                    + self.get_comm_bus_cost_decrease_reward(message)\n",
    "                )\n",
    "        \n",
    "        if len(self.message_pool) == 0 and len(self.comm_bus) == 0:\n",
    "            done = True\n",
    "        \n",
    "        info = {\n",
    "            \"message_pool\": self.message_pool,\n",
    "            \"comm_bus\": self.comm_bus,\n",
    "        }\n",
    "        return self.get_observation(), reward, done, info\n",
    "    \n",
    "    def render(self, mode='console'):\n",
    "        if mode == 'console':\n",
    "            print(\"Message Pool: \", self.message_pool)\n",
    "            print(\"Communication Bus: \", self.comm_bus)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = EnvConfig()\n",
    "# print(env_config.max_message_pool, env_config.comm_bus_size)\n",
    "reward_config = RewardConfig()\n",
    "env = MessageNetworkEnv(env_config, reward_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/somoysu/anaconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'spaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m env \u001b[39m=\u001b[39m make_vec_env(\u001b[39mlambda\u001b[39;00m: env, n_envs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Initializes the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m PPO(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mMultiInputPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     env,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     tensorboard_log\u001b[39m=\u001b[39;49mlogdir,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Trains the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/somoysu/Documents/Ediss/DIE/CADES/CADES2/cades-drl/src/message_passing_somoy.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:164\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_kl \u001b[39m=\u001b[39m target_kl\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m _init_setup_model:\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_model()\n",
      "File \u001b[0;32m~/anaconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:167\u001b[0m, in \u001b[0;36mPPO._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_model()\n\u001b[1;32m    169\u001b[0m     \u001b[39m# Initialize schedules for policy/value clipping\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_range \u001b[39m=\u001b[39m get_schedule_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_range)\n",
      "File \u001b[0;32m~/anaconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:123\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrollout_buffer \u001b[39m=\u001b[39m buffer_cls(\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps,\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     n_envs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs,\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[39m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_class(  \u001b[39m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_space, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_schedule, use_sde\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_sde, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_kwargs\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[39m# pytype:enable=not-instantiable\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/policies.py:853\u001b[0m, in \u001b[0;36mMultiInputActorCriticPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     observation_space: spaces\u001b[39m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     optimizer_kwargs: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ):\n\u001b[0;32m--> 853\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    854\u001b[0m         observation_space,\n\u001b[1;32m    855\u001b[0m         action_space,\n\u001b[1;32m    856\u001b[0m         lr_schedule,\n\u001b[1;32m    857\u001b[0m         net_arch,\n\u001b[1;32m    858\u001b[0m         activation_fn,\n\u001b[1;32m    859\u001b[0m         ortho_init,\n\u001b[1;32m    860\u001b[0m         use_sde,\n\u001b[1;32m    861\u001b[0m         log_std_init,\n\u001b[1;32m    862\u001b[0m         full_std,\n\u001b[1;32m    863\u001b[0m         use_expln,\n\u001b[1;32m    864\u001b[0m         squash_output,\n\u001b[1;32m    865\u001b[0m         features_extractor_class,\n\u001b[1;32m    866\u001b[0m         features_extractor_kwargs,\n\u001b[1;32m    867\u001b[0m         share_features_extractor,\n\u001b[1;32m    868\u001b[0m         normalize_images,\n\u001b[1;32m    869\u001b[0m         optimizer_class,\n\u001b[1;32m    870\u001b[0m         optimizer_kwargs,\n\u001b[1;32m    871\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/policies.py:481\u001b[0m, in \u001b[0;36mActorCriticPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mortho_init \u001b[39m=\u001b[39m ortho_init\n\u001b[1;32m    480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_features_extractor \u001b[39m=\u001b[39m share_features_extractor\n\u001b[0;32m--> 481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_features_extractor()\n\u001b[1;32m    482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor\u001b[39m.\u001b[39mfeatures_dim\n\u001b[1;32m    483\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_features_extractor:\n",
      "File \u001b[0;32m~/anaconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/policies.py:120\u001b[0m, in \u001b[0;36mBaseModel.make_features_extractor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_features_extractor\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseFeaturesExtractor:\n\u001b[1;32m    119\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Helper method to create a features extractor.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/torch_layers.py:259\u001b[0m, in \u001b[0;36mCombinedExtractor.__init__\u001b[0;34m(self, observation_space, cnn_output_dim, normalized_image)\u001b[0m\n\u001b[1;32m    256\u001b[0m extractors: Dict[\u001b[39mstr\u001b[39m, nn\u001b[39m.\u001b[39mModule] \u001b[39m=\u001b[39m {}\n\u001b[1;32m    258\u001b[0m total_concat_size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 259\u001b[0m \u001b[39mfor\u001b[39;00m key, subspace \u001b[39min\u001b[39;00m observation_space\u001b[39m.\u001b[39;49mspaces\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    260\u001b[0m     \u001b[39mif\u001b[39;00m is_image_space(subspace, normalized_image\u001b[39m=\u001b[39mnormalized_image):\n\u001b[1;32m    261\u001b[0m         extractors[key] \u001b[39m=\u001b[39m NatureCNN(subspace, features_dim\u001b[39m=\u001b[39mcnn_output_dim, normalized_image\u001b[39m=\u001b[39mnormalized_image)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'spaces'"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "logdir = f\"../logs/{int(time.time())}/\"\n",
    "models_dir = f\"../models/{int(time.time())}/\"\n",
    "\n",
    "\n",
    "\n",
    "# Wraps the environment with a vectorized environment\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# Initializes the model\n",
    "model = PPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=logdir,\n",
    "    batch_size=128,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Trains the model\n",
    "model.learn(total_timesteps=10)\n",
    "\n",
    "# Saves the model\n",
    "model.save(models_dir + f\"model_{int(time.time())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cades",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
