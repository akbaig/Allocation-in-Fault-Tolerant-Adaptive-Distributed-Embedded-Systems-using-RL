{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we declare the Environment we should shape our Message Object model and environment config model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Each Message Object contains the following attributes:\n",
    "    size: size of the message\n",
    "    source: source task [Task where the message is generated]\n",
    "    destination: destination task [Task where the message is meant to be sent]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MessageObject:\n",
    "    def __init__(self, size, source, destination):\n",
    "        self.size = size\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "\n",
    "    def to_array(self):\n",
    "        return [self.size, self.source, self.destination]\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Contains name and scaling factors of all the reward functions\n",
    "  Negative rewards:\n",
    "    message_pool_invalid_index_reward: when the message pool is accessed with an invalid index\n",
    "    comm_bus_invalid_index_reward: when the communication bus is accessed with an invalid index\n",
    "    comm_bus_overflow_reward: when the communication bus is overflowed\n",
    "  Positive rewards:\n",
    "    message_store_reward: when a message is stored in the message pool\n",
    "    message_send_reward: when a message is sent from the message pool to the communication bus\n",
    "    message_pool_cost_decrease_reward: when the message pool cost decreases\n",
    "    comm_bus_cost_decrease_reward: when the communication bus cost decreases\n",
    "'''\n",
    "class RewardConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_pool_invalid_index=-1,\n",
    "        comm_bus_invalid_index=-1,\n",
    "        comm_bus_overflow=-1,\n",
    "        message_pool_no_change_reward = -0.1,\n",
    "        comm_bus_no_change_reward = -0.1,\n",
    "        message_store=1,\n",
    "        message_send=1,\n",
    "        message_pool_cost_decrease=1,\n",
    "        comm_bus_cost_decrease=1,\n",
    "    ):\n",
    "        # Negative rewards\n",
    "        self.message_pool_invalid_index_reward = message_pool_invalid_index\n",
    "        self.message_pool_no_change_reward = message_pool_no_change_reward\n",
    "        self.comm_bus_invalid_index_reward = comm_bus_invalid_index\n",
    "        self.comm_bus_overflow_reward = comm_bus_overflow\n",
    "        self.comm_bus_no_change_reward = comm_bus_no_change_reward\n",
    "        # Positive rewards\n",
    "        self.message_store_reward = message_store\n",
    "        self.message_send_reward = message_send\n",
    "        self.message_pool_cost_decrease_reward = message_pool_cost_decrease\n",
    "        self.comm_bus_cost_decrease_reward = comm_bus_cost_decrease\n",
    "\n",
    "    def load_from_file(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            self.message_pool_invalid_index_reward = data.get(\n",
    "                \"message_pool_invalid_index_reward\",\n",
    "                self.message_pool_invalid_index_reward,\n",
    "            )\n",
    "            self.comm_bus_invalid_index_reward = data.get(\n",
    "                \"comm_bus_invalid_index_reward\", self.comm_bus_invalid_index_reward\n",
    "            )\n",
    "            self.comm_bus_overflow_reward = data.get(\n",
    "                \"comm_bus_overflow_reward\", self.comm_bus_overflow_reward\n",
    "            )\n",
    "            self.message_store_reward = data.get(\n",
    "                \"message_store_reward\", self.message_store_reward\n",
    "            )\n",
    "            self.message_send_reward = data.get(\n",
    "                \"message_send_reward\", self.message_send_reward\n",
    "            )\n",
    "            self.message_pool_cost_decrease_reward = data.get(\n",
    "                \"message_pool_cost_decrease_reward\",\n",
    "                self.message_pool_cost_decrease_reward,\n",
    "            )\n",
    "            self.comm_bus_cost_decrease_reward = data.get(\n",
    "                \"comm_bus_cost_decrease_reward\", self.comm_bus_cost_decrease_reward\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "Environment Configuration\n",
    "    min_message_pool: minimum number of messages in the message pool\n",
    "    max_message_pool: maximum number of messages in the message pool\n",
    "    min_message_obj_size: minimum size of the message object\n",
    "    max_message_obj_size: maximum size of the message object\n",
    "    comm_bus_size: size of the communication bus\n",
    "\n",
    "    Can directly load from a json file or manual user input\n",
    "    Can be used to generate a random environment configuration\n",
    "'''\n",
    "class EnvConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_message_pool=10,\n",
    "        max_message_pool=25,\n",
    "        min_message_obj_size=50,\n",
    "        max_message_obj_size=300,\n",
    "        comm_bus_size=1500,\n",
    "    ):\n",
    "        self.min_message_pool = min_message_pool\n",
    "        self.max_message_pool = max_message_pool\n",
    "        self.min_message_obj_size = min_message_obj_size\n",
    "        self.max_message_obj_size = max_message_obj_size\n",
    "        self.comm_bus_size = comm_bus_size\n",
    "\n",
    "    def load_from_file(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            self.min_message_pool = data.get(\"min_message_pool\", self.min_message_pool)\n",
    "            self.max_message_pool = data.get(\"max_message_pool\", self.max_message_pool)\n",
    "            self.min_message_obj_size = data.get(\n",
    "                \"min_message_obj_size\", self.min_message_obj_size\n",
    "            )\n",
    "            self.max_message_obj_size = data.get(\n",
    "                \"max_message_obj_size\", self.max_message_obj_size\n",
    "            )\n",
    "            self.comm_bus_size = data.get(\"comm_bus_size\", self.comm_bus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "The Message Network Environment for CADES-Task 2 (Message Passing)\n",
    "    Observation Space:\n",
    "        3 - as each message is represented by 3 values (size, source, destination)\n",
    "        self.config.max_message_pool + self.config.comm_bus_size - as the message pool\n",
    "        and communication bus are both in observation space\n",
    "    Action Space:\n",
    "        [OPERATION, MESSAGE_POOL_INDEX, COMM_BUS_INDEX]\n",
    "        OPERATION: 0 - Indicates the node manager/Agent pick message from the message pool and queues in the bus. ,\n",
    "                   1 - Indicates the node manager/Agent pick message from the communication bus and sends it to destination node.\n",
    "        MESSAGE_POOL_INDEX: Index of the message object in the 'message pool' to be picked for sending \n",
    "        COMM_BUS_INDEX: Index of the message object in the 'communication bus' to be picked for sending to destination  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MessageNetworkEnv(gym.Env):\n",
    "    def __init__(self, envconfig, reward_config):\n",
    "        super(MessageNetworkEnv, self).__init__()\n",
    "\n",
    "        self.config = envconfig\n",
    "        self.reward_config = reward_config\n",
    "\n",
    "        # Observation Space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"message_pool\": spaces.Box(low=0, high=self.config.max_message_obj_size, shape=(3, self.config.max_message_pool), dtype=np.float32),\n",
    "            \"comm_bus\": spaces.Box(low=0, high=self.config.max_message_obj_size, shape=(3, self.config.comm_bus_size), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "        # Action Space\n",
    "        self.action_space = spaces.MultiDiscrete(\n",
    "            [2, self.config.max_message_pool, self.config.max_message_pool]\n",
    "        )\n",
    "\n",
    "        self.message_pool = []\n",
    "        self.comm_bus = []\n",
    "\n",
    "    def reset(self):\n",
    "        num_messages = np.random.randint(\n",
    "            self.config.min_message_pool, self.config.max_message_pool + 1\n",
    "        )\n",
    "        self.message_pool = [self.generate_message() for _ in range(num_messages)]\n",
    "        self.comm_bus = []\n",
    "\n",
    "        observation = self.get_observation()\n",
    "        \n",
    "        return observation\n",
    "\n",
    "    # Generates Message Objects\n",
    "    # Source and Destination tasks are now assigned at random, but we will have to change this later\n",
    "    def generate_message(self):\n",
    "        size = np.random.uniform(\n",
    "            self.config.min_message_obj_size / self.config.comm_bus_size,\n",
    "            self.config.max_message_obj_size / self.config.comm_bus_size,\n",
    "        )\n",
    "        source = np.random.randint(0, 100)\n",
    "        destination = np.random.randint(0, 100)\n",
    "\n",
    "        while (\n",
    "            destination == source\n",
    "        ):  # To ensure Source and destination are never the same\n",
    "            destination = np.random.randint(0, 100)\n",
    "\n",
    "        return MessageObject(size, source, destination)\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are used to convert the message pool and communication bus into an observation\n",
    "    \"\"\"\n",
    "\n",
    "    def get_message_pool_observation(self):\n",
    "        message_pool_observation = np.zeros(\n",
    "            (3, self.config.max_message_pool), dtype=np.float32\n",
    "        )\n",
    "        for i, message in enumerate(self.message_pool):\n",
    "            message_pool_observation[:, i] = message.to_array()\n",
    "        return message_pool_observation\n",
    "\n",
    "    def get_comm_bus_observation(self):\n",
    "        comm_bus_observation = np.zeros(\n",
    "            (3, self.config.comm_bus_size), dtype=np.float32\n",
    "        )\n",
    "        for i, message in enumerate(self.comm_bus):\n",
    "            comm_bus_observation[:, i] = message.to_array()\n",
    "        return comm_bus_observation\n",
    "\n",
    "    def get_observation(self):\n",
    "        return {\n",
    "            \"message_pool\": self.get_message_pool_observation(),\n",
    "            \"comm_bus\": self.get_comm_bus_observation(),\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are used to convert the action into a message object\n",
    "    \"\"\"\n",
    "\n",
    "    def get_message_from_message_pool(self, message_pool_index):\n",
    "        if 0 <= message_pool_index < len(self.message_pool):\n",
    "            return self.message_pool[message_pool_index]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_message_from_comm_bus(self, comm_bus_index):\n",
    "        if 0 <= comm_bus_index < len(self.comm_bus):\n",
    "            return self.comm_bus[comm_bus_index]\n",
    "        else: \n",
    "            return None\n",
    "\n",
    "    def get_message_from_action(self, action):\n",
    "        operation, message_pool_index, comm_bus_index = action\n",
    "        if operation == 0:\n",
    "            return self.get_message_from_message_pool(message_pool_index)\n",
    "        elif operation == 1:\n",
    "            return self.get_message_from_comm_bus(comm_bus_index)\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are used to generate appropriate negative rewards based on observations\n",
    "        - MESSAGE_POOL_INVALID_INDEX_REWARD if it chooses an index that is not in the message pool.\n",
    "        - COMM_BUS_INVALID_INDEX_REWARD if it chooses an index that is not in the communication bus.\n",
    "\n",
    "        - COMM_BUS_OVERFLOW_REWARD if it tries to place a message object in a occupied communication bus \n",
    "        that makes it surpass its capacity.\n",
    "        \n",
    "        - MESSAGE_POOL_NO_CHANGE_REWARD if OPERATION=1 AND message objects are in the message pool \n",
    "        AND adequate space is available in the communication bus \n",
    "\n",
    "        - COMM_BUS_NO_CHANGE_REWARD if OPERATION=0 AND message objects in the communication bus. \n",
    "\n",
    "        ==== The below rewards are to be done later =====\n",
    "        -? MESSAGE_POOL_INVALID_INDEX_REWARD if it chooses a previously picked index from the message pool. \n",
    "        -? COMM_BUS_INVALID_INDEX_REWARD if it chooses a previously picked index from the communication bus.\n",
    "        - SENDER_TASK_NOT_ALLOCATED_REWARD if the sender task is not allocated to the node manager/Agent.\n",
    "        - RECEIVER_TASK_NOT_ALLOCATED_REWARD if the receiver task is not allocated to the node manager/Agent. \n",
    "    \"\"\"\n",
    "\n",
    "    def check_message_pool_invalid_index(self, message_pool_index):\n",
    "        if message_pool_index >= len(self.message_pool):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_comm_bus_invalid_index(self, comm_bus_index):\n",
    "        if comm_bus_index >= len(self.comm_bus):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_comm_bus_overflow(self, message):\n",
    "        if (\n",
    "            sum(message.size for message in self.comm_bus) + message.size\n",
    "            > self.config.comm_bus_size\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_message_pool_no_change(self, message):\n",
    "        if message in self.message_pool:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_comm_bus_no_change(self, message):\n",
    "        if message in self.comm_bus:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are used to generate appropriate positive rewards based on observations\n",
    "        - MESSAGE_STORE_REWARD if it chooses OPERATION=0 AND message objects are in the message pool\n",
    "        - MESSAGE_SEND_REWARD if it chooses OPERATION=1 AND message objects are in the communication bus\n",
    "        - MESSAGE_POOL_COST_DECREASE_REWARD if it chooses OPERATION=0 AND messages are in the message pool \n",
    "        AND the size of MESSAGE_POOL decreases. [Reward assigned based on the size of the message chosen]\n",
    "        - COMM_BUS_COST_DECREASE_REWARD if it chooses OPERATION=1 AND messages are in the communication bus\n",
    "        AND the size of COMM_BUS decreases. [Reward assigned based on the size of the message chosen]\n",
    "\n",
    "\n",
    "        ==== The below rewards are to be done later =====\n",
    "        -? MESSAGE_TRAVEL_DISTANCE_REWARD if OPERATION=1 AND ...\n",
    "    \"\"\"\n",
    "\n",
    "    def check_message_store_reward(self, message):\n",
    "        if message in self.message_pool:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_message_send_reward(self, message):\n",
    "        if message in self.comm_bus:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_message_pool_cost_decrease_reward(self, message):\n",
    "        message_size_range = (\n",
    "            self.config.max_message_obj_size - self.config.min_message_obj_size\n",
    "        )\n",
    "        normalized_size = (\n",
    "            message.size * self.config.comm_bus_size - self.config.min_message_obj_size\n",
    "        ) / message_size_range\n",
    "        reward = normalized_size * self.reward_config.message_pool_cost_decrease_reward\n",
    "        return reward\n",
    "\n",
    "    def get_comm_bus_cost_decrease_reward(self, message):\n",
    "        message_size_range = (\n",
    "            self.config.max_message_obj_size - self.config.min_message_obj_size\n",
    "        )\n",
    "        normalized_size = (\n",
    "            message.size * self.config.comm_bus_size - self.config.min_message_obj_size\n",
    "        ) / message_size_range\n",
    "        reward = normalized_size * self.reward_config.comm_bus_cost_decrease_reward\n",
    "        return reward\n",
    "\n",
    "    \"\"\"\n",
    "        The following functions are for the step function\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        operation, message_pool_index, comm_bus_index = action\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        if operation == 0:\n",
    "            message = self.get_message_from_action(action)\n",
    "            if self.check_message_pool_invalid_index(message_pool_index):\n",
    "                reward += self.reward_config.message_pool_invalid_index_reward\n",
    "            elif self.check_comm_bus_overflow(message):\n",
    "                reward += self.reward_config.comm_bus_overflow_reward\n",
    "            elif self.check_message_pool_no_change(message):\n",
    "                reward += self.reward_config.message_pool_no_change_reward\n",
    "            else:\n",
    "                self.message_pool.remove(message)\n",
    "                self.comm_bus.append(message)\n",
    "                reward = (\n",
    "                    self.reward_config.message_store_reward\n",
    "                    + self.get_message_pool_cost_decrease_reward(message)\n",
    "                )\n",
    "        elif operation == 1:\n",
    "            message = self.get_message_from_action(action)\n",
    "            if self.check_comm_bus_invalid_index(comm_bus_index):\n",
    "                reward += self.reward_config.comm_bus_invalid_index_reward\n",
    "            elif self.check_comm_bus_no_change(message):\n",
    "                reward += self.reward_config.comm_bus_no_change_reward\n",
    "            else:\n",
    "                self.comm_bus.remove(message)\n",
    "                reward = (\n",
    "                    self.reward_config.message_send_reward\n",
    "                    + self.get_comm_bus_cost_decrease_reward(message)\n",
    "                )\n",
    "        \n",
    "        if len(self.message_pool) == 0 and len(self.comm_bus) == 0:\n",
    "            done = True\n",
    "        \n",
    "        info = {\n",
    "            \"message_pool\": self.message_pool,\n",
    "            \"comm_bus\": self.comm_bus,\n",
    "        }\n",
    "        return self.get_observation(), reward, done, info\n",
    "    \n",
    "    def render(self, mode='console'):\n",
    "        if mode == 'console':\n",
    "            print(\"Message Pool: \", self.message_pool)\n",
    "            print(\"Communication Bus: \", self.comm_bus)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env_config = EnvConfig()\n",
    "# print(env_config.max_message_pool, env_config.comm_bus_size)\n",
    "reward_config = RewardConfig()\n",
    "env = MessageNetworkEnv(env_config, reward_config)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_action = env.action_space.sample()  # Generates a sample action\n",
    "observation, reward, done, info = env.step(test_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ../logs/1700694123/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 387  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ediss5/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:278: UserWarning: Path '../models/1700694123' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "logdir = f\"../logs/{int(time.time())}/\"\n",
    "models_dir = f\"../models/{int(time.time())}/\"\n",
    "\n",
    "\n",
    "# Initializes the model\n",
    "model = PPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=logdir,\n",
    "    batch_size=128,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Trains the model\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Saves the model\n",
    "model.save(models_dir + f\"model_{int(time.time())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=models_dir,\n",
    "                             log_path=logdir, eval_freq=10000,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../logs/1700694123/ppo_Test_0\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 398   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 5     |\n",
      "|    total_timesteps | 24096 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 323        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 26144      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02073974 |\n",
      "|    clip_fraction        | 0.0827     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.41      |\n",
      "|    explained_variance   | -0.000276  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.991      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    value_loss           | 6.68       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 303         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 28192       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022300452 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.513       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 295         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 30240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023332868 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.536       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 1.21        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ediss5/somoy/cades-drl/src/message_passing_somoy.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlops2/home/ediss5/somoy/cades-drl/src/message_passing_somoy.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwhile\u001b[39;00m iters\u001b[39m<\u001b[39mEPOCHS:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlops2/home/ediss5/somoy/cades-drl/src/message_passing_somoy.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     iters \u001b[39m=\u001b[39m iters\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmlops2/home/ediss5/somoy/cades-drl/src/message_passing_somoy.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mTIMESTEPS, reset_num_timesteps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, tb_log_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mppo_Test\u001b[39;49m\u001b[39m\"\u001b[39;49m,callback\u001b[39m=\u001b[39;49meval_callback)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmlops2/home/ediss5/somoy/cades-drl/src/message_passing_somoy.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodels_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00miters\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    308\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    309\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    310\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    311\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    312\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    313\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    314\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    244\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    246\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 248\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:181\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    180\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m--> 181\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:100\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:447\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39m# Reset success rate buffer\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_success_buffer \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 447\u001b[0m episode_rewards, episode_lengths \u001b[39m=\u001b[39m evaluate_policy(\n\u001b[1;32m    448\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    449\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_env,\n\u001b[1;32m    450\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    451\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender,\n\u001b[1;32m    452\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeterministic,\n\u001b[1;32m    453\u001b[0m     return_episode_rewards\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    454\u001b[0m     warn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn,\n\u001b[1;32m    455\u001b[0m     callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_success_callback,\n\u001b[1;32m    456\u001b[0m )\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluations_timesteps\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((env\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[0;32m---> 88\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(observations, state\u001b[39m=\u001b[39;49mstates, episode_start\u001b[39m=\u001b[39;49mepisode_starts, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m     89\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[1;32m     90\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:535\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    521\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    522\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/policies.py:343\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    340\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    342\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 343\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m    344\u001b[0m \u001b[39m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    345\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/policies.py:687\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, observation: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    680\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[39m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_distribution(observation)\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/policies.py:721\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39mGet the current policy distribution given the observations.\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \n\u001b[1;32m    717\u001b[0m \u001b[39m:param obs:\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[39m:return: the action distribution.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    720\u001b[0m features \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi_features_extractor)\n\u001b[0;32m--> 721\u001b[0m latent_pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_extractor\u001b[39m.\u001b[39;49mforward_actor(features)\n\u001b[1;32m    722\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/stable_baselines3/common/torch_layers.py:266\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_actor\u001b[39m(\u001b[39mself\u001b[39m, features: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 266\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_net(features))\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cades/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "is_train = True\n",
    "EPOCHS=5\n",
    "if is_train:\n",
    "    TIMESTEPS = 25000\n",
    "    iters=0\n",
    "    while iters<EPOCHS:\n",
    "        iters = iters+1\n",
    "\n",
    "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"ppo_Test\",callback=eval_callback)\n",
    "        model.save(f\"{models_dir}/{iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cades",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
