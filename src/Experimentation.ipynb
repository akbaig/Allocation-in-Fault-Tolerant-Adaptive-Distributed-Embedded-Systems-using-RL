{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8551c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO,DQN,A2C\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import numpy as np\n",
    "from rl_env import StatesGenerator, get_benchmark_rewards,compute_reward, critical_task_reward, CustomEnv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b79063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config\n",
    "config, _ = get_config()\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "logdir = f\"../logs/{int(time.time())}/\"\n",
    "models_dir = f\"../models/{int(time.time())}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c50300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.55073586, 0.45003873, 0.14639814, 0.44848954, 0.5120062 ,\n",
       "       0.51432998, 0.22385747, 0.49883811, 0.20681642, 0.40975988,\n",
       "       1.        , 0.92796282, 0.64910922, 0.75290473])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=CustomEnv(config)\n",
    "check_env(env)\n",
    "model = PPO('MlpPolicy', env, verbose=1,tensorboard_log=logdir,batch_size=64)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf05920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=models_dir,\n",
    "                             log_path=logdir, eval_freq=10000,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd13a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Training\n",
    "is_train = False\n",
    "if is_train:\n",
    "    TIMESTEPS = 25_000\n",
    "    iters = 0\n",
    "    while True:\n",
    "        iters += 1\n",
    "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO_Testing_new_bin_penalty\",callback=eval_callback)\n",
    "        model.save(f\"{models_dir}/{TIMESTEPS * iters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9f85f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-13.75\n",
      "mean_episode_len:2.95\n",
      "defaultdict(<class 'int'>, {'DUBLICATE_PICK': 67, 'BIN_OVERFLOW': 33})\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, env, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    all_episode_rewards = []\n",
    "    all_episodes_len=[]\n",
    "    termination_cause=defaultdict(int)\n",
    "    is_success=0\n",
    "\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            if info['is_success']:\n",
    "                is_success+=1\n",
    "            if done:\n",
    "                termination_cause[info['termination_cause']]+=1\n",
    "                all_episodes_len.append(info['episode_len'])\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "\n",
    "    return mean_episode_reward,(sum(all_episodes_len)/num_episodes),termination_cause,is_success\n",
    "\n",
    "mean_reward,mean_episode_len,termination_cause,is_success=evaluate(model, env,100)\n",
    "print(f\"mean_reward:{mean_reward:.2f}\")\n",
    "print(f\"mean_episode_len:{mean_episode_len:.2f}\")\n",
    "print(termination_cause)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba9b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "loaded_model =PPO.load('..\\\\models\\\\1674683760\\\\1000000', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36189bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:6.21\n",
      "mean_episode_len:6.35\n",
      "defaultdict(<class 'int'>, {'SUCCESS': 34, 'BIN_OVERFLOW': 66})\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "# evaluation over N episodes\n",
    "mean_reward,mean_episode_len,termination_cause,is_success=evaluate(loaded_model, env,100)\n",
    "print(f\"mean_reward:{mean_reward:.2f}\")\n",
    "print(f\"mean_episode_len:{mean_episode_len:.2f}\")\n",
    "print(termination_cause)\n",
    "print(is_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0801bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task capacities: [0.34077381 0.26488095 0.58705357 0.40178571 0.57738095 0.3296131\n",
      " 0.2202381  0.36160714 0.43154762 0.49925595] All tasks sum:  4.014136904761905\n",
      "Node capacities: [0.8764881  1.         0.83630952 0.62127976]\n",
      "[4 0]\n",
      "[8 1]\n",
      "[9 2]\n",
      "[2 3]\n",
      "[3 1]\n",
      "[7 0]\n",
      "\n",
      "{'is_success': False, 'episode_len': 5, 'termination_cause': 'BIN_OVERFLOW', 'assignment_status': [[4], [8, 3], [9], [2]]}\n",
      "\n",
      "Assignment= [[4], [8, 3], [9], [2]]\n",
      "\n",
      "Last Action: Allocating Task # 7 in Node/Bin # 0\n",
      "\n",
      "Tasks after episodes: [0.34077381 0.26488095 0.         0.         0.         0.3296131\n",
      " 0.2202381  0.36160714 0.         0.        ]\n",
      "Remaining Node capacities: [0.29910714 0.16666667 0.33705357 0.03422619]\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "obs=env.reset()\n",
    "\n",
    "FACTOR=1\n",
    "\n",
    "print(\"Task capacities:\",obs[0:10]*FACTOR,\"All tasks sum: \",sum(obs[0:10])*FACTOR)\n",
    "print(\"Node capacities:\",obs[10:]*FACTOR)\n",
    "\n",
    "while not done:\n",
    "    # _states are only useful when using LSTM policies\n",
    "    action, _states = loaded_model.predict(obs)\n",
    "    # here, action, rewards and dones are arrays\n",
    "    # because we are using vectorized env\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "print()\n",
    "print(info)\n",
    "print()\n",
    "print(\"Assignment=\",info['assignment_status'])\n",
    "print()\n",
    "print(f\"Last Action: Allocating Task # {action[0]} in Node/Bin # {action[1]}\")\n",
    "print()\n",
    "\n",
    "print(\"Tasks after episodes:\",obs[0:10]*FACTOR)\n",
    "print(\"Remaining Node capacities:\",obs[10:]*FACTOR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe0c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_node_capacities=obs[10:]*FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_occupancy=round(100-np.mean((np.array(remaining_node_capacities) /np.array([800,1000,1200,1400,1600])))*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a36281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
