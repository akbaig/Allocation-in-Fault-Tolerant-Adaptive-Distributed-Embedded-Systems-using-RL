{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8551c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO,DQN,A2C\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import numpy as np\n",
    "from rl_env import StatesGenerator, get_benchmark_rewards,compute_reward, critical_task_reward, CustomEnv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b79063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config\n",
    "config, _ = get_config()\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "logdir = f\"../logs/{int(time.time())}/\"\n",
    "models_dir = f\"../models/{int(time.time())}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c50300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.56428571, 0.33714286, 0.54571429, 0.28071429, 0.565     ,\n",
       "       0.21571429, 0.36285714, 0.55357143, 0.11214286, 0.33928571,\n",
       "       0.57142857, 0.71428571, 0.85714286, 1.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=CustomEnv(config)\n",
    "check_env(env)\n",
    "model = PPO('MlpPolicy', env, verbose=1,tensorboard_log=logdir,batch_size=64)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf05920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=models_dir,\n",
    "                             log_path=logdir, eval_freq=10000,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd13a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Training\n",
    "is_train = False\n",
    "if is_train:\n",
    "    TIMESTEPS = 25_000\n",
    "    iters = 0\n",
    "    while True:\n",
    "        iters += 1\n",
    "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO_Testing_new_bin_penalty\",callback=eval_callback)\n",
    "        model.save(f\"{models_dir}/{TIMESTEPS * iters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a9f85f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-12.00\n",
      "mean_episode_len:3.00\n",
      "defaultdict(<class 'int'>, {'DUBLICATE_PICK': 1, 'BIN_OVERFLOW': 1})\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, env, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    all_episode_rewards = []\n",
    "    all_episodes_len=[]\n",
    "    termination_cause=defaultdict(int)\n",
    "    is_success=0\n",
    "    occupancy_ratio=[]\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        total_bins = obs[10:]*1400\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            if info['is_success']:\n",
    "                is_success+=1\n",
    "                remaining_node_capacities=obs[10:]*1400\n",
    "                occupancy_ratio.append(round(100-np.mean((np.array(remaining_node_capacities) /np.array(total_bins)))*100,2))\n",
    "              \n",
    "            if done:\n",
    "                termination_cause[info['termination_cause']]+=1\n",
    "                all_episodes_len.append(info['episode_len'])\n",
    "            \n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "\n",
    "    return mean_episode_reward,(sum(all_episodes_len)/num_episodes),termination_cause,is_success,occupancy_ratio\n",
    "\n",
    "mean_reward,mean_episode_len,termination_cause,is_success,_=evaluate(model, env,2)\n",
    "print(f\"mean_reward:{mean_reward:.2f}\")\n",
    "print(f\"mean_episode_len:{mean_episode_len:.2f}\")\n",
    "print(termination_cause)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ba9b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "loaded_model =PPO.load('..\\\\models\\\\1674683760\\\\1125000', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c1b7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36189bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:5.63\n",
      "mean_episode_len:7.51\n",
      "defaultdict(<class 'int'>, {'SUCCESS': 28, 'BIN_OVERFLOW': 72})\n",
      "Average Occupancy ratio: 87.76321428571428\n",
      "Average time per input: 0.003847689628601074\n"
     ]
    }
   ],
   "source": [
    "# evaluation over N episodes\n",
    "t1=time.time()\n",
    "mean_reward,mean_episode_len,termination_cause,is_success,avg_occupancy_ratio=evaluate(loaded_model, env,100)\n",
    "print(f\"mean_reward:{mean_reward:.2f}\")\n",
    "print(f\"mean_episode_len:{mean_episode_len:.2f}\")\n",
    "print(termination_cause)\n",
    "print(\"Average Occupancy ratio:\",np.array(avg_occupancy_ratio).mean())\n",
    "print(\"Average time per input:\",(time.time()-t1)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0801bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task capacities: [430. 492. 298. 385. 242. 563. 199. 476. 773. 365.] All tasks sum:  4222.999999999999\n",
      "Node capacities: [ 800. 1000. 1200. 1400.]\n",
      "\n",
      "{'is_success': False, 'episode_len': 9, 'termination_cause': 'BIN_OVERFLOW', 'assignment_status': [[8], [7, 0], [1, 3, 6], [5, 9, 2]]}\n",
      "\n",
      "Assignment= [[8], [7, 0], [1, 3, 6], [5, 9, 2]]\n",
      "\n",
      "Last Action: Allocating Task # 4 in Node/Bin # 3\n",
      "\n",
      "Tasks after episodes: [  0.   0.   0.   0. 242.   0.   0.   0.   0.   0.]\n",
      "Remaining Node capacities: [ 27.  94. 124. 174.]\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "obs=env.reset()\n",
    "\n",
    "FACTOR=1400\n",
    "\n",
    "print(\"Task Costs:\",obs[0:10]*FACTOR,\"All tasks sum: \",sum(obs[0:10])*FACTOR)\n",
    "print(\"Node capacities:\",obs[10:]*FACTOR)\n",
    "\n",
    "while not done:\n",
    "    # _states are only useful when using LSTM policies\n",
    "    action, _states = loaded_model.predict(obs)\n",
    "    # here, action, rewards and dones are arrays\n",
    "    # because we are using vectorized env\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "print()\n",
    "print(info)\n",
    "print()\n",
    "print(\"Assignment=\",info['assignment_status'])\n",
    "print()\n",
    "print(f\"Last Action: Allocating Task # {action[0]} in Node/Bin # {action[1]}\")\n",
    "print()\n",
    "\n",
    "print(\"Tasks after episodes:\",obs[0:10]*FACTOR)\n",
    "print(\"Remaining Node capacities:\",obs[10:]*FACTOR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe0c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_node_capacities=obs[10:]*FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_occupancy=round(100-np.mean((np.array(remaining_node_capacities) /np.array([800,1000,1200,1400,1600])))*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a36281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
