{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8551c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO,DQN,A2C\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import numpy as np\n",
    "from states_generator import StatesGenerator\n",
    "from rl_env import get_benchmark_rewards,compute_reward, critical_task_reward\n",
    "from cades_env import CadesEnv\n",
    "from collections import defaultdict\n",
    "from utils import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b79063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config\n",
    "config, _ = get_config()\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "logdir = f\"../logs/{int(time.time())}/\"\n",
    "models_dir = f\"../models/{int(time.time())}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c50300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tasks_total_cost': 5204.0,\n",
       " 'nodes_total_capacity': 6400.0,\n",
       " 'extra_capacity': 19.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=CadesEnv(config)\n",
    "check_env(env)\n",
    "model = PPO('MultiInputPolicy', env, verbose=1,tensorboard_log=logdir,batch_size=128,device='cuda')\n",
    "env.reset()\n",
    "env.get_env_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf05920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=models_dir,\n",
    "                             log_path=logdir, eval_freq=10000,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd13a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../logs/1677948409/ppo_Test_0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.36     |\n",
      "|    ep_rew_mean     | -14.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 493      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.85         |\n",
      "|    ep_rew_mean          | -14.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071161725 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.69        |\n",
      "|    explained_variance   | -0.00427     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.2         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00849     |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 4.24         |\n",
      "|    ep_rew_mean          | -13.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076111443 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.68        |\n",
      "|    explained_variance   | -0.069       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.8         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    value_loss           | 46.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.08        |\n",
      "|    ep_rew_mean          | -13.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 500         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012233006 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.67       |\n",
      "|    explained_variance   | -0.0305     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.68        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Success rate: 0.00%\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2           |\n",
      "|    mean_reward          | -9          |\n",
      "|    success_rate         | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011519097 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.65       |\n",
      "|    explained_variance   | -0.00571    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0344     |\n",
      "|    value_loss           | 19.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaheryar\\Desktop\\EDISS\\Semester 1\\DIE 1\\bin-packing-drl\\conditional_bin_packing\\venv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.1      |\n",
      "|    ep_rew_mean     | -13      |\n",
      "| time/              |          |\n",
      "|    fps             | 506      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.4         |\n",
      "|    ep_rew_mean          | -12.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 513         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010881149 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.62       |\n",
      "|    explained_variance   | -0.00327    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.97        |\n",
      "|    ep_rew_mean          | -11.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010020243 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.57       |\n",
      "|    explained_variance   | -0.00155    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 22.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.03        |\n",
      "|    ep_rew_mean          | -11         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 525         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012576275 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.51       |\n",
      "|    explained_variance   | -0.0011     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 22          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.7         |\n",
      "|    ep_rew_mean          | -10.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 526         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010728188 |\n",
      "|    clip_fraction        | 0.097       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.45       |\n",
      "|    explained_variance   | -0.000781   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.73        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 21.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-8.80 +/- 0.40\n",
      "Episode length: 2.20 +/- 0.40\n",
      "Success rate: 0.00%\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.2         |\n",
      "|    mean_reward          | -8.8        |\n",
      "|    success_rate         | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012419142 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.39       |\n",
      "|    explained_variance   | -0.000348   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.54     |\n",
      "|    ep_rew_mean     | -10.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 528      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.92        |\n",
      "|    ep_rew_mean          | -9.98       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 530         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009891143 |\n",
      "|    clip_fraction        | 0.0743      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.33       |\n",
      "|    explained_variance   | -0.000347   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.78        |\n",
      "|    ep_rew_mean          | -9.02       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 528         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009878302 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.27       |\n",
      "|    explained_variance   | -0.0003     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.14        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run Training\n",
    "is_train = True\n",
    "EPOCHS=10\n",
    "if is_train:\n",
    "    TIMESTEPS = 25000\n",
    "    iters=0\n",
    "    while iters<EPOCHS:\n",
    "        iters = iters+1\n",
    "\n",
    "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"ppo_Test\",callback=eval_callback)\n",
    "        model.save(f\"{models_dir}/{iters}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ba9b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "loaded_model =PPO.load('..\\\\models\\\\1677681542\\\\50000', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a9f85f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-8.35\n",
      "mean_episode_len:2.35\n",
      "defaultdict(<class 'int'>, {'BIN_OVERFLOW': 93, 'DUBLICATE_PICK': 7})\n"
     ]
    }
   ],
   "source": [
    "mean_reward,mean_episode_len,termination_cause,is_success,_=evaluate(loaded_model, env,100)\n",
    "print(f\"mean_reward:{mean_reward:.2f}\")\n",
    "print(f\"mean_episode_len:{mean_episode_len:.2f}\")\n",
    "print(termination_cause)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c1b7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36189bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:5.11\n",
      "mean_episode_len:6.80\n",
      "defaultdict(<class 'int'>, {'SUCCESS': 29, 'BIN_OVERFLOW': 70, 'DUBLICATE_PICK': 1})\n",
      "Average Occupancy ratio: 76.4451724137931\n",
      "Average time per input: 0.003796532154083252\n"
     ]
    }
   ],
   "source": [
    "# evaluation over N episodes\n",
    "t1=time.time()\n",
    "mean_reward,mean_episode_len,termination_cause,is_success,avg_occupancy_ratio=evaluate(loaded_model, env,100)\n",
    "print(f\"mean_reward:{mean_reward:.2f}\")\n",
    "print(f\"mean_episode_len:{mean_episode_len:.2f}\")\n",
    "print(termination_cause)\n",
    "print(\"Average Occupancy ratio:\",np.array(avg_occupancy_ratio).mean())\n",
    "print(\"Average time per input:\",(time.time()-t1)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0801bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Costs: [103. 271. 685. 588. 277. 605. 205. 302. 411. 626.] All tasks sum:  4073.0000000000005\n",
      "Node capacities: [1000. 1800. 1800. 1400.]\n",
      "[2 1]\n",
      "Remaining Node capacities: [1000. 1115. 1800. 1400.]\n",
      "\n",
      "[3 2]\n",
      "Remaining Node capacities: [1000. 1115. 1212. 1400.]\n",
      "\n",
      "[9 3]\n",
      "Remaining Node capacities: [1000. 1115. 1212.  774.]\n",
      "\n",
      "[5 1]\n",
      "Remaining Node capacities: [1000.  510. 1212.  774.]\n",
      "\n",
      "[8 2]\n",
      "Remaining Node capacities: [1000.  510.  801.  774.]\n",
      "\n",
      "[1 0]\n",
      "Remaining Node capacities: [729. 510. 801. 774.]\n",
      "\n",
      "[7 0]\n",
      "Remaining Node capacities: [427. 510. 801. 774.]\n",
      "\n",
      "[4 3]\n",
      "Remaining Node capacities: [427. 510. 801. 497.]\n",
      "\n",
      "[6 2]\n",
      "Remaining Node capacities: [427. 510. 596. 497.]\n",
      "\n",
      "[0 2]\n",
      "Remaining Node capacities: [427. 510. 493. 497.]\n",
      "\n",
      "\n",
      "{'is_success': True, 'episode_len': 10, 'termination_cause': 'SUCCESS', 'assignment_status': [[1, 7], [2, 5], [3, 8, 6, 0], [9, 4]]}\n",
      "\n",
      "Assignment= [[1, 7], [2, 5], [3, 8, 6, 0], [9, 4]]\n",
      "\n",
      "Last Action: Allocating Task # 0 in Node/Bin # 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "obs=env.reset()\n",
    "\n",
    "FACTOR=env.norm_factor\n",
    "\n",
    "print(\"Task Costs:\",obs['tasks']*FACTOR,\"All tasks sum: \",sum(obs['tasks'])*FACTOR)\n",
    "print(\"Node capacities:\",obs['nodes']*FACTOR)\n",
    "\n",
    "while not done:\n",
    "    # _states are only useful when using LSTM policies\n",
    "    action, _states = loaded_model.predict(obs)\n",
    "    # here, action, rewards and dones are arrays\n",
    "    # because we are using vectorized env\n",
    "    obs, reward, done, info = env.step(action)\n",
    "#     print(\"Tasks after episodes:\",obs['tasks']*FACTOR)\n",
    "    print(action)\n",
    "    print(\"Remaining Node capacities:\",obs['nodes']*FACTOR)\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print(info)\n",
    "print()\n",
    "print(\"Assignment=\",info['assignment_status'])\n",
    "print()\n",
    "print(f\"Last Action: Allocating Task # {action[0]} in Node/Bin # {action[1]}\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbe0c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_node_capacities=obs['nodes']*FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_occupancy=round(100-np.mean((np.array(remaining_node_capacities) /np.array([800,1000,1200,1400,1600])))*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a36281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
