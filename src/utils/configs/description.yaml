# Experiment parameters
experiment_name: "Experiment name for MLflow"
run_name: "Run name for MLflow"
# Environment parameters
min_task_size: "Minimum task size"
max_task_size: "Maximum task size"
min_num_tasks: "Minimum number of tasks"
max_num_tasks: "Maximum number of tasks"
min_node_size: "Minimum node size"
max_node_size: "Maximum node size"
min_num_nodes: "Minimum number of nodes"
max_num_nodes: "Maximum number of nodes"
number_of_critical_tasks: "Number of critical tasks"
number_of_replicas: "Number of replicas of critical tasks"
min_num_comms: "Minimum number of communications"
max_num_comms: "Maximum number of communications"
max_comm_chain: "Maximum length of chained communication"
non_critical_comm: "Non-critical tasks communication"
critical_comm: "Critical tasks and replicas communication"
use_comm_graph_in_train: "Use communication graph in training"
invalid_action_replacement: "Replace invalid actions"
# Reward parameters
SUCCESS_reward: "Success reward"
DUPLICATE_PICK_reward: "Duplicate pick reward"
NODE_OVERFLOW_reward: "Node overflow reward"
STEP_reward: "Step reward"
BONUS_reward: "Bonus reward"
CRITICAL_reward: "Critical task reward"
DUPLICATE_CRITICAL_PICK_reward: "Duplicate critical task reward"
COMM_reward: "Total communication reward"
# Model parameters
seed: "Random seed"
epochs: "Number of episodes"
eval_timesteps: "Number of timesteps after which model is evaluated"
batch_size: "Batch size"
lr: "Initial learning rate"
alpha: "Alpha value to compute reward"
device: "Device to use (if no GPU available, value should be 'cpu')"
inference: "Evaluate the model"
verbose: "Debugging mode"

