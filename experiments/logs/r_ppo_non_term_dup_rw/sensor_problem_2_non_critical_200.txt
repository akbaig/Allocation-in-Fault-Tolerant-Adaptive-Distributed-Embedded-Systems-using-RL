Epoch # 1
Eval num_timesteps=10000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.4        |
|    mean_reward          | -4.46      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.02832235 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.19      |
|    explained_variance   | 0.118      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.6       |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 56         |
----------------------------------------
New best mean reward!
Epoch # 2
Eval num_timesteps=20000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -5.23       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.019306675 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.17       |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.1        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0404     |
|    value_loss           | 38.5        |
-----------------------------------------
Epoch # 3
Eval num_timesteps=30000, episode_reward=-4.45 +/- 3.05
Episode length: 2.40 +/- 0.80
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.4         |
|    mean_reward          | -4.45       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.012396574 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.21       |
|    explained_variance   | 0.217       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.8        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 24.4        |
-----------------------------------------
New best mean reward!
Epoch # 4
Eval num_timesteps=40000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.4         |
|    mean_reward          | -4.46       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.021961555 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.6        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 36.4        |
-----------------------------------------
Epoch # 5
Eval num_timesteps=50000, episode_reward=-3.69 +/- 1.54
Episode length: 2.20 +/- 0.40
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2.2       |
|    mean_reward          | -3.69     |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0270236 |
|    clip_fraction        | 0.159     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.25     |
|    explained_variance   | 0.212     |
|    learning_rate        | 0.0003    |
|    loss                 | 11.6      |
|    n_updates            | 3900      |
|    policy_gradient_loss | -0.0393   |
|    value_loss           | 30.2      |
---------------------------------------
New best mean reward!
Epoch # 6
Eval num_timesteps=60000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -5.23       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015734293 |
|    clip_fraction        | 0.0961      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.17       |
|    explained_variance   | 0.0408      |
|    learning_rate        | 0.0003      |
|    loss                 | 22.2        |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 54.4        |
-----------------------------------------
Epoch # 7
Eval num_timesteps=70000, episode_reward=-9.02 +/- 2.99
Episode length: 3.60 +/- 0.80
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3.6          |
|    mean_reward          | -9.02        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0015551243 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.29        |
|    explained_variance   | 0.0832       |
|    learning_rate        | 0.0003       |
|    loss                 | 24           |
|    n_updates            | 5460         |
|    policy_gradient_loss | -0.0056      |
|    value_loss           | 98.9         |
------------------------------------------
Epoch # 8
Eval num_timesteps=80000, episode_reward=-2.92 +/- 0.00
Episode length: 2.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2          |
|    mean_reward          | -2.92      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.01856099 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.1       |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.0003     |
|    loss                 | 10         |
|    n_updates            | 6240       |
|    policy_gradient_loss | -0.0363    |
|    value_loss           | 20         |
----------------------------------------
New best mean reward!
Epoch # 9
Eval num_timesteps=90000, episode_reward=-3.69 +/- 1.54
Episode length: 2.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.2         |
|    mean_reward          | -3.69       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.015492221 |
|    clip_fraction        | 0.0727      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.18       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 7030        |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 37.9        |
-----------------------------------------
Epoch # 10
Eval num_timesteps=100000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -5.23       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.005801833 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.28       |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 7810        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 28.2        |
-----------------------------------------
Epoch # 11
Eval num_timesteps=110000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.4         |
|    mean_reward          | -4.46       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.010876756 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.35       |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.6        |
|    n_updates            | 8590        |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 24.4        |
-----------------------------------------
Epoch # 12
Eval num_timesteps=120000, episode_reward=-0.76 +/- 1.03
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -0.76       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.009337847 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.29       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.9        |
|    n_updates            | 9370        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 82.3        |
-----------------------------------------
New best mean reward!
Epoch # 13
Eval num_timesteps=130000, episode_reward=-0.34 +/- 1.03
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.4          |
|    mean_reward          | -0.34        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0052037425 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.26        |
|    explained_variance   | 0.481        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.88         |
|    n_updates            | 10150        |
|    policy_gradient_loss | -0.0183      |
|    value_loss           | 17.4         |
------------------------------------------
New best mean reward!
Epoch # 14
Eval num_timesteps=140000, episode_reward=-6.71 +/- 4.79
Episode length: 3.00 +/- 1.26
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3            |
|    mean_reward          | -6.71        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0022963057 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.0003       |
|    loss                 | 42.4         |
|    n_updates            | 10930        |
|    policy_gradient_loss | -0.0116      |
|    value_loss           | 73.2         |
------------------------------------------
Epoch # 15
Eval num_timesteps=150000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0031407564 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.26        |
|    explained_variance   | 0.451        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.2         |
|    n_updates            | 11710        |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 43.1         |
------------------------------------------
Epoch # 16
Eval num_timesteps=160000, episode_reward=-8.28 +/- 1.85
Episode length: 3.40 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.4         |
|    mean_reward          | -8.28       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.012095346 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 0.219       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.79        |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 26.7        |
-----------------------------------------
Epoch # 17
Eval num_timesteps=170000, episode_reward=0.23 +/- 0.51
Episode length: 3.80 +/- 0.40
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3.8          |
|    mean_reward          | 0.23         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0019283267 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 0.103        |
|    learning_rate        | 0.0003       |
|    loss                 | 19.8         |
|    n_updates            | 13280        |
|    policy_gradient_loss | -0.0086      |
|    value_loss           | 41.2         |
------------------------------------------
New best mean reward!
Epoch # 18
Eval num_timesteps=180000, episode_reward=-5.22 +/- 3.05
Episode length: 2.60 +/- 0.80
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.22        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 180000       |
| train/                  |              |
|    approx_kl            | 0.0037167575 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.22        |
|    explained_variance   | 0.427        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.9         |
|    n_updates            | 14060        |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 22.4         |
------------------------------------------
Epoch # 19
Eval num_timesteps=190000, episode_reward=-2.35 +/- 1.51
Episode length: 3.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.2         |
|    mean_reward          | -2.35       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.004605246 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.00297     |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 14840       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 51.3        |
-----------------------------------------
Epoch # 20
Eval num_timesteps=200000, episode_reward=-6.20 +/- 1.14
Episode length: 3.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3            |
|    mean_reward          | -6.2         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0072878674 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0003       |
|    loss                 | 28.7         |
|    n_updates            | 15620        |
|    policy_gradient_loss | -0.0138      |
|    value_loss           | 41.3         |
------------------------------------------
Epoch # 21
Eval num_timesteps=210000, episode_reward=-4.58 +/- 2.34
Episode length: 3.00 +/- 1.26
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3            |
|    mean_reward          | -4.58        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0034062522 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.0003       |
|    loss                 | 19.3         |
|    n_updates            | 16400        |
|    policy_gradient_loss | -0.0121      |
|    value_loss           | 35           |
------------------------------------------
Epoch # 22
Eval num_timesteps=220000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.4          |
|    mean_reward          | -4.46        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 220000       |
| train/                  |              |
|    approx_kl            | 0.0035690102 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.7         |
|    n_updates            | 17180        |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 27.2         |
------------------------------------------
Epoch # 23
Eval num_timesteps=230000, episode_reward=-1.07 +/- 1.82
Episode length: 2.80 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.8         |
|    mean_reward          | -1.07       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.002776958 |
|    clip_fraction        | 0.00313     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.35       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.6        |
|    n_updates            | 17960       |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 44.3        |
-----------------------------------------
Epoch # 24
Eval num_timesteps=240000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0036514876 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0003       |
|    loss                 | 19.5         |
|    n_updates            | 18740        |
|    policy_gradient_loss | -0.0163      |
|    value_loss           | 47.1         |
------------------------------------------
Epoch # 25
Eval num_timesteps=250000, episode_reward=-1.02 +/- 1.34
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.4          |
|    mean_reward          | -1.02        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0018986189 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.0819       |
|    learning_rate        | 0.0003       |
|    loss                 | 43.2         |
|    n_updates            | 19530        |
|    policy_gradient_loss | -0.00635     |
|    value_loss           | 101          |
------------------------------------------
Epoch # 26
Eval num_timesteps=260000, episode_reward=-6.00 +/- 1.54
Episode length: 2.80 +/- 0.40
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.8          |
|    mean_reward          | -6           |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 260000       |
| train/                  |              |
|    approx_kl            | 0.0018616351 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.22        |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0003       |
|    loss                 | 29.9         |
|    n_updates            | 20310        |
|    policy_gradient_loss | -0.00704     |
|    value_loss           | 75.5         |
------------------------------------------
Epoch # 27
Eval num_timesteps=270000, episode_reward=-8.58 +/- 1.70
Episode length: 3.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.6         |
|    mean_reward          | -8.58       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.001955858 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.36       |
|    explained_variance   | 0.00247     |
|    learning_rate        | 0.0003      |
|    loss                 | 13.7        |
|    n_updates            | 21090       |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 29.2        |
-----------------------------------------
Epoch # 28
Eval num_timesteps=280000, episode_reward=-2.92 +/- 1.09
Episode length: 3.40 +/- 0.80
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.4         |
|    mean_reward          | -2.92       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.003564436 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.25       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.6        |
|    n_updates            | 21870       |
|    policy_gradient_loss | -0.00958    |
|    value_loss           | 92.7        |
-----------------------------------------
Epoch # 29
Eval num_timesteps=290000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 290000       |
| train/                  |              |
|    approx_kl            | 0.0015996215 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.0812       |
|    learning_rate        | 0.0003       |
|    loss                 | 34.1         |
|    n_updates            | 22650        |
|    policy_gradient_loss | -0.00668     |
|    value_loss           | 53.1         |
------------------------------------------
Epoch # 30
Eval num_timesteps=300000, episode_reward=-3.89 +/- 1.49
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.4         |
|    mean_reward          | -3.89       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.008674427 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.32       |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0003      |
|    loss                 | 9.78        |
|    n_updates            | 23430       |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 44.3        |
-----------------------------------------
Epoch # 31
Eval num_timesteps=310000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.6        |
|    mean_reward          | -5.23      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.01692205 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.34      |
|    explained_variance   | 0.241      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.7       |
|    n_updates            | 24210      |
|    policy_gradient_loss | -0.0283    |
|    value_loss           | 22.9       |
----------------------------------------
Epoch # 32
Eval num_timesteps=320000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -5.23       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.002789888 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.6        |
|    n_updates            | 24990       |
|    policy_gradient_loss | -0.00974    |
|    value_loss           | 45.7        |
-----------------------------------------
Epoch # 33
Eval num_timesteps=330000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -5.23       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.005444028 |
|    clip_fraction        | 0.00313     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.0003      |
|    loss                 | 32.1        |
|    n_updates            | 25780       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 49          |
-----------------------------------------
Epoch # 34
Eval num_timesteps=340000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0018357118 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0003       |
|    loss                 | 35.4         |
|    n_updates            | 26560        |
|    policy_gradient_loss | -0.00626     |
|    value_loss           | 59.4         |
------------------------------------------
Epoch # 35
Eval num_timesteps=350000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.4         |
|    mean_reward          | -4.46       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.007921498 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.23       |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.0003      |
|    loss                 | 7.19        |
|    n_updates            | 27340       |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 16.6        |
-----------------------------------------
Epoch # 36
Eval num_timesteps=360000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.4           |
|    mean_reward          | -4.46         |
|    success_rate         | 0             |
| time/                   |               |
|    total_timesteps      | 360000        |
| train/                  |               |
|    approx_kl            | 0.00040786155 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.33         |
|    explained_variance   | 0.211         |
|    learning_rate        | 0.0003        |
|    loss                 | 28            |
|    n_updates            | 28120         |
|    policy_gradient_loss | -0.00486      |
|    value_loss           | 71.6          |
-------------------------------------------
Epoch # 37
Eval num_timesteps=370000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -5.23       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.010230897 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0.163       |
|    learning_rate        | 0.0003      |
|    loss                 | 26          |
|    n_updates            | 28900       |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 57.7        |
-----------------------------------------
Epoch # 38
Eval num_timesteps=380000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 380000       |
| train/                  |              |
|    approx_kl            | 0.0020782962 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.0998       |
|    learning_rate        | 0.0003       |
|    loss                 | 27.2         |
|    n_updates            | 29680        |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 53.4         |
------------------------------------------
Epoch # 39
Eval num_timesteps=390000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0019967817 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | -0.048       |
|    learning_rate        | 0.0003       |
|    loss                 | 34.4         |
|    n_updates            | 30460        |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 53.8         |
------------------------------------------
Epoch # 40
Eval num_timesteps=400000, episode_reward=-5.22 +/- 3.05
Episode length: 2.60 +/- 0.80
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.22        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0002902262 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.0003       |
|    loss                 | 45.2         |
|    n_updates            | 31240        |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 101          |
------------------------------------------
Epoch # 41
Eval num_timesteps=410000, episode_reward=-3.69 +/- 1.54
Episode length: 2.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.2         |
|    mean_reward          | -3.69       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.013178484 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.21       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.2        |
|    n_updates            | 32030       |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 27.6        |
-----------------------------------------
Epoch # 42
Eval num_timesteps=420000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2.4        |
|    mean_reward          | -4.46      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.00496641 |
|    clip_fraction        | 0.00547    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.27      |
|    explained_variance   | 0.229      |
|    learning_rate        | 0.0003     |
|    loss                 | 79.3       |
|    n_updates            | 32810      |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 94.9       |
----------------------------------------
Epoch # 43
Eval num_timesteps=430000, episode_reward=-8.28 +/- 1.85
Episode length: 3.40 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3.4          |
|    mean_reward          | -8.28        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 430000       |
| train/                  |              |
|    approx_kl            | 0.0007055071 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.0003       |
|    loss                 | 26.7         |
|    n_updates            | 33590        |
|    policy_gradient_loss | -0.00717     |
|    value_loss           | 44.1         |
------------------------------------------
Epoch # 44
Eval num_timesteps=440000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 440000       |
| train/                  |              |
|    approx_kl            | 0.0004391456 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0.305        |
|    learning_rate        | 0.0003       |
|    loss                 | 18.2         |
|    n_updates            | 34370        |
|    policy_gradient_loss | -0.00517     |
|    value_loss           | 41.5         |
------------------------------------------
Epoch # 45
Eval num_timesteps=450000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.4          |
|    mean_reward          | -4.46        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 450000       |
| train/                  |              |
|    approx_kl            | 0.0027147303 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.32        |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0003       |
|    loss                 | 68.4         |
|    n_updates            | 35150        |
|    policy_gradient_loss | -0.00635     |
|    value_loss           | 114          |
------------------------------------------
Epoch # 46
Eval num_timesteps=460000, episode_reward=-6.74 +/- 3.41
Episode length: 3.00 +/- 0.89
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3            |
|    mean_reward          | -6.74        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 460000       |
| train/                  |              |
|    approx_kl            | 0.0008827485 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.4         |
|    n_updates            | 35930        |
|    policy_gradient_loss | -0.00853     |
|    value_loss           | 48.7         |
------------------------------------------
Epoch # 47
Eval num_timesteps=470000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.4         |
|    mean_reward          | -4.46       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.005115675 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.5        |
|    n_updates            | 36710       |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 45.5        |
-----------------------------------------
Epoch # 48
Eval num_timesteps=480000, episode_reward=-7.23 +/- 4.08
Episode length: 3.40 +/- 0.80
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3.4          |
|    mean_reward          | -7.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0015056129 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.36        |
|    explained_variance   | -0.0666      |
|    learning_rate        | 0.0003       |
|    loss                 | 118          |
|    n_updates            | 37490        |
|    policy_gradient_loss | -0.00403     |
|    value_loss           | 150          |
------------------------------------------
Epoch # 49
Eval num_timesteps=490000, episode_reward=-3.09 +/- 3.25
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.4          |
|    mean_reward          | -3.09        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 0.0037269364 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.46         |
|    n_updates            | 38280        |
|    policy_gradient_loss | -0.00999     |
|    value_loss           | 45           |
------------------------------------------
Epoch # 50
Eval num_timesteps=500000, episode_reward=-4.20 +/- 2.16
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -4.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.006054954 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.3        |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.79        |
|    n_updates            | 39060       |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 32.3        |
-----------------------------------------
Epoch # 51
Eval num_timesteps=510000, episode_reward=-3.01 +/- 2.30
Episode length: 2.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.2         |
|    mean_reward          | -3.01       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.006687532 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.27       |
|    explained_variance   | 0.101       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.4        |
|    n_updates            | 39840       |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 46          |
-----------------------------------------
Epoch # 52
Eval num_timesteps=520000, episode_reward=-5.22 +/- 3.16
Episode length: 3.60 +/- 1.02
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.6         |
|    mean_reward          | -5.22       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.011429403 |
|    clip_fraction        | 0.0555      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.21       |
|    explained_variance   | 0.082       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.2        |
|    n_updates            | 40620       |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 53.7        |
-----------------------------------------
Epoch # 53
Eval num_timesteps=530000, episode_reward=-5.30 +/- 2.09
Episode length: 3.60 +/- 1.02
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.6         |
|    mean_reward          | -5.3        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 530000      |
| train/                  |             |
|    approx_kl            | 0.013581013 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.18       |
|    explained_variance   | -0.0526     |
|    learning_rate        | 0.0003      |
|    loss                 | 31.5        |
|    n_updates            | 41400       |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 53.7        |
-----------------------------------------
Epoch # 54
Eval num_timesteps=540000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -5.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 540000       |
| train/                  |              |
|    approx_kl            | 0.0028123353 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 0.408        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.8         |
|    n_updates            | 42180        |
|    policy_gradient_loss | -0.00765     |
|    value_loss           | 24           |
------------------------------------------
Epoch # 55
Eval num_timesteps=550000, episode_reward=-0.76 +/- 1.03
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.6          |
|    mean_reward          | -0.76        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 550000       |
| train/                  |              |
|    approx_kl            | 0.0059368927 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.0003       |
|    loss                 | 17.6         |
|    n_updates            | 42960        |
|    policy_gradient_loss | -0.00947     |
|    value_loss           | 48.2         |
------------------------------------------
Epoch # 56
Eval num_timesteps=560000, episode_reward=-5.23 +/- 1.89
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -5.23       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.007202105 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.28       |
|    explained_variance   | 0.0969      |
|    learning_rate        | 0.0003      |
|    loss                 | 10.2        |
|    n_updates            | 43740       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 36.5        |
-----------------------------------------
Epoch # 57
Eval num_timesteps=570000, episode_reward=-4.68 +/- 2.39
Episode length: 3.40 +/- 1.20
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 3.4        |
|    mean_reward          | -4.68      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 570000     |
| train/                  |            |
|    approx_kl            | 0.02199848 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.26      |
|    explained_variance   | 0.191      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.8       |
|    n_updates            | 44530      |
|    policy_gradient_loss | -0.0242    |
|    value_loss           | 53.3       |
----------------------------------------
Epoch # 58
Eval num_timesteps=580000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 2.4          |
|    mean_reward          | -4.46        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 580000       |
| train/                  |              |
|    approx_kl            | 0.0043800534 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.0003       |
|    loss                 | 29.3         |
|    n_updates            | 45310        |
|    policy_gradient_loss | -0.0139      |
|    value_loss           | 37.9         |
------------------------------------------
Epoch # 59
Eval num_timesteps=590000, episode_reward=-9.45 +/- 4.38
Episode length: 4.00 +/- 1.26
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | -9.45        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0020365482 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0.398        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.2         |
|    n_updates            | 46090        |
|    policy_gradient_loss | -0.00638     |
|    value_loss           | 34.1         |
------------------------------------------
Epoch # 60
Eval num_timesteps=600000, episode_reward=-2.40 +/- 2.04
Episode length: 3.00 +/- 0.63
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3            |
|    mean_reward          | -2.4         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 600000       |
| train/                  |              |
|    approx_kl            | 0.0140665155 |
|    clip_fraction        | 0.148        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.26        |
|    explained_variance   | 0.413        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.7         |
|    n_updates            | 46870        |
|    policy_gradient_loss | -0.0261      |
|    value_loss           | 21.9         |
------------------------------------------
Epoch # 61
Eval num_timesteps=610000, episode_reward=-4.46 +/- 1.89
Episode length: 2.40 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.4         |
|    mean_reward          | -4.46       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 610000      |
| train/                  |             |
|    approx_kl            | 0.007653225 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.27       |
|    explained_variance   | 0.0877      |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 47650       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 35.6        |
-----------------------------------------
Epoch # 62
Eval num_timesteps=620000, episode_reward=-2.92 +/- 0.00
Episode length: 2.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2           |
|    mean_reward          | -2.92       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 620000      |
| train/                  |             |
|    approx_kl            | 0.020028573 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.05       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0003      |
|    loss                 | 38          |
|    n_updates            | 48430       |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 49.6        |
-----------------------------------------
Epoch # 63
Eval num_timesteps=630000, episode_reward=-7.08 +/- 2.22
Episode length: 4.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4.2         |
|    mean_reward          | -7.08       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 630000      |
| train/                  |             |
|    approx_kl            | 0.010264358 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.7        |
|    n_updates            | 49210       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 42.1        |
-----------------------------------------
Epoch # 64
Eval num_timesteps=640000, episode_reward=-2.67 +/- 3.18
Episode length: 3.20 +/- 1.17
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3.2          |
|    mean_reward          | -2.67        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 640000       |
| train/                  |              |
|    approx_kl            | 0.0046669086 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.0003       |
|    loss                 | 11.5         |
|    n_updates            | 49990        |
|    policy_gradient_loss | -0.00836     |
|    value_loss           | 32.6         |
------------------------------------------
Epoch # 65
Eval num_timesteps=650000, episode_reward=-5.00 +/- 2.56
Episode length: 3.80 +/- 1.60
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.8         |
|    mean_reward          | -5          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.026958475 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.1        |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.3        |
|    n_updates            | 50780       |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 23.4        |
-----------------------------------------
Epoch # 66
Eval num_timesteps=660000, episode_reward=-5.64 +/- 2.43
Episode length: 3.80 +/- 0.75
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 3.8          |
|    mean_reward          | -5.64        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 660000       |
| train/                  |              |
|    approx_kl            | 0.0023933388 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | -0.182       |
|    learning_rate        | 0.0003       |
|    loss                 | 14           |
|    n_updates            | 51560        |
|    policy_gradient_loss | -0.00495     |
|    value_loss           | 128          |
------------------------------------------
Epoch # 67
Eval num_timesteps=670000, episode_reward=-5.69 +/- 5.66
Episode length: 4.60 +/- 2.80
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4.6         |
|    mean_reward          | -5.69       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.009418106 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.17       |
|    explained_variance   | 0.00475     |
|    learning_rate        | 0.0003      |
|    loss                 | 40.9        |
|    n_updates            | 52340       |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 59.1        |
-----------------------------------------
Epoch # 68
Eval num_timesteps=680000, episode_reward=-1.79 +/- 2.66
Episode length: 2.60 +/- 0.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.6         |
|    mean_reward          | -1.79       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.011550123 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.13       |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    n_updates            | 53120       |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 30.1        |
-----------------------------------------
Epoch # 69
Eval num_timesteps=690000, episode_reward=0.83 +/- 0.41
Episode length: 3.20 +/- 1.94
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.2         |
|    mean_reward          | 0.83        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 690000      |
| train/                  |             |
|    approx_kl            | 0.010329109 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.15       |
|    explained_variance   | 0.0426      |
|    learning_rate        | 0.0003      |
|    loss                 | 21.7        |
|    n_updates            | 53900       |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 73.1        |
-----------------------------------------
New best mean reward!
Epoch # 70
Eval num_timesteps=700000, episode_reward=-5.74 +/- 4.40
Episode length: 4.80 +/- 2.40
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4.8          |
|    mean_reward          | -5.74        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 700000       |
| train/                  |              |
|    approx_kl            | 0.0068392386 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.06        |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.0003       |
|    loss                 | 42.2         |
|    n_updates            | 54680        |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 127          |
------------------------------------------
Epoch # 71
Eval num_timesteps=710000, episode_reward=-4.98 +/- 2.09
Episode length: 4.60 +/- 1.36
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4.6         |
|    mean_reward          | -4.98       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 710000      |
| train/                  |             |
|    approx_kl            | 0.004823751 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.09       |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.22        |
|    n_updates            | 55460       |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 47.1        |
-----------------------------------------
Epoch # 72
Eval num_timesteps=720000, episode_reward=1.82 +/- 1.17
Episode length: 3.60 +/- 1.36
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.6         |
|    mean_reward          | 1.82        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.010407506 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.11       |
|    explained_variance   | 0.21        |
|    learning_rate        | 0.0003      |
|    loss                 | 35.9        |
|    n_updates            | 56240       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 54.7        |
-----------------------------------------
New best mean reward!
Epoch # 73
Eval num_timesteps=730000, episode_reward=-0.84 +/- 3.25
Episode length: 4.40 +/- 1.02
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4.4         |
|    mean_reward          | -0.835      |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 730000      |
| train/                  |             |
|    approx_kl            | 0.009460814 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.1        |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.1        |
|    n_updates            | 57030       |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 62          |
-----------------------------------------
Epoch # 74
Eval num_timesteps=740000, episode_reward=-3.16 +/- 4.26
Episode length: 4.60 +/- 3.72
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4.6         |
|    mean_reward          | -3.16       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 740000      |
| train/                  |             |
|    approx_kl            | 0.017127804 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    n_updates            | 57810       |
|    policy_gradient_loss | -0.0407     |
|    value_loss           | 36.2        |
-----------------------------------------
Epoch # 75
Eval num_timesteps=750000, episode_reward=-0.69 +/- 2.57
Episode length: 4.60 +/- 2.15
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4.6         |
|    mean_reward          | -0.695      |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.018229885 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.01       |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.5        |
|    n_updates            | 58590       |
|    policy_gradient_loss | -0.0447     |
|    value_loss           | 34.6        |
-----------------------------------------
Epoch # 76
Eval num_timesteps=760000, episode_reward=-3.80 +/- 7.16
Episode length: 5.40 +/- 2.65
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 5.4         |
|    mean_reward          | -3.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.010180026 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.09       |
|    explained_variance   | 0.0167      |
|    learning_rate        | 0.0003      |
|    loss                 | 37.6        |
|    n_updates            | 59370       |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 72.1        |
-----------------------------------------
Epoch # 77
Eval num_timesteps=770000, episode_reward=-6.63 +/- 6.62
Episode length: 7.60 +/- 2.65
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 7.6         |
|    mean_reward          | -6.63       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 770000      |
| train/                  |             |
|    approx_kl            | 0.005447278 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.1        |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 60150       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 40.5        |
-----------------------------------------
Epoch # 78
Eval num_timesteps=780000, episode_reward=-12.84 +/- 3.79
Episode length: 8.00 +/- 1.10
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8           |
|    mean_reward          | -12.8       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 780000      |
| train/                  |             |
|    approx_kl            | 0.012128927 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.96       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.1        |
|    n_updates            | 60930       |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 48.9        |
-----------------------------------------
Epoch # 79
Eval num_timesteps=790000, episode_reward=-2.14 +/- 3.65
Episode length: 5.40 +/- 2.15
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 5.4         |
|    mean_reward          | -2.14       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 790000      |
| train/                  |             |
|    approx_kl            | 0.008658031 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.7        |
|    n_updates            | 61710       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 189         |
-----------------------------------------
Epoch # 80
Eval num_timesteps=800000, episode_reward=-1.30 +/- 1.42
Episode length: 5.40 +/- 1.85
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 5.4         |
|    mean_reward          | -1.3        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.012793431 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.94       |
|    explained_variance   | 0.138       |
|    learning_rate        | 0.0003      |
|    loss                 | 33          |
|    n_updates            | 62490       |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 56.5        |
-----------------------------------------
Epoch # 81
Eval num_timesteps=810000, episode_reward=-3.24 +/- 1.99
Episode length: 5.00 +/- 1.10
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 5           |
|    mean_reward          | -3.24       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 810000      |
| train/                  |             |
|    approx_kl            | 0.009540053 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.97       |
|    explained_variance   | 0.237       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.2        |
|    n_updates            | 63280       |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 59.7        |
-----------------------------------------
Epoch # 82
Eval num_timesteps=820000, episode_reward=0.42 +/- 2.46
Episode length: 6.40 +/- 2.06
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 6.4         |
|    mean_reward          | 0.425       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 820000      |
| train/                  |             |
|    approx_kl            | 0.015190968 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.97       |
|    explained_variance   | 0.00354     |
|    learning_rate        | 0.0003      |
|    loss                 | 25.6        |
|    n_updates            | 64060       |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 39.5        |
-----------------------------------------
Epoch # 83
Eval num_timesteps=830000, episode_reward=0.39 +/- 5.91
Episode length: 8.20 +/- 2.04
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8.2         |
|    mean_reward          | 0.385       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 830000      |
| train/                  |             |
|    approx_kl            | 0.017850198 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.78       |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.1        |
|    n_updates            | 64840       |
|    policy_gradient_loss | -0.0379     |
|    value_loss           | 44.6        |
-----------------------------------------
Epoch # 84
Eval num_timesteps=840000, episode_reward=4.77 +/- 3.24
Episode length: 7.00 +/- 1.67
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 7           |
|    mean_reward          | 4.77        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.022032259 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.65       |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.1        |
|    n_updates            | 65620       |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 32.8        |
-----------------------------------------
New best mean reward!
Epoch # 85
Eval num_timesteps=850000, episode_reward=3.65 +/- 2.92
Episode length: 6.20 +/- 1.17
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 6.2          |
|    mean_reward          | 3.65         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 850000       |
| train/                  |              |
|    approx_kl            | 0.0054800953 |
|    clip_fraction        | 0.257        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.56        |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.0003       |
|    loss                 | 23.4         |
|    n_updates            | 66400        |
|    policy_gradient_loss | -0.0203      |
|    value_loss           | 65           |
------------------------------------------
Epoch # 86
Eval num_timesteps=860000, episode_reward=3.38 +/- 4.90
Episode length: 8.20 +/- 3.25
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8.2         |
|    mean_reward          | 3.38        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 860000      |
| train/                  |             |
|    approx_kl            | 0.011861445 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.38       |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.7        |
|    n_updates            | 67180       |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 29.8        |
-----------------------------------------
Epoch # 87
Eval num_timesteps=870000, episode_reward=0.92 +/- 2.64
Episode length: 4.60 +/- 0.80
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4.6         |
|    mean_reward          | 0.925       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 870000      |
| train/                  |             |
|    approx_kl            | 0.012276264 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.47       |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.88        |
|    n_updates            | 67960       |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 31.7        |
-----------------------------------------
Epoch # 88
Eval num_timesteps=880000, episode_reward=-8.79 +/- 7.19
Episode length: 10.20 +/- 4.17
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10.2        |
|    mean_reward          | -8.79       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.020711258 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.9        |
|    n_updates            | 68740       |
|    policy_gradient_loss | -0.0458     |
|    value_loss           | 50.5        |
-----------------------------------------
Epoch # 89
Eval num_timesteps=890000, episode_reward=4.26 +/- 5.84
Episode length: 7.40 +/- 1.50
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 7.4       |
|    mean_reward          | 4.26      |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 890000    |
| train/                  |           |
|    approx_kl            | 0.0419292 |
|    clip_fraction        | 0.277     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.42     |
|    explained_variance   | 0.142     |
|    learning_rate        | 0.0003    |
|    loss                 | 71.8      |
|    n_updates            | 69530     |
|    policy_gradient_loss | -0.0168   |
|    value_loss           | 92.9      |
---------------------------------------
Epoch # 90
Eval num_timesteps=900000, episode_reward=1.87 +/- 2.61
Episode length: 8.00 +/- 2.76
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8           |
|    mean_reward          | 1.87        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.024213124 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.06       |
|    explained_variance   | 0.157       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.3        |
|    n_updates            | 70310       |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 45.6        |
-----------------------------------------
Eval num_timesteps=910000, episode_reward=1.19 +/- 3.77
Episode length: 8.60 +/- 4.27
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 8.6          |
|    mean_reward          | 1.19         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 910000       |
| train/                  |              |
|    approx_kl            | 0.0074250447 |
|    clip_fraction        | 0.204        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.1         |
|    explained_variance   | 0.00192      |
|    learning_rate        | 0.0003       |
|    loss                 | 56.7         |
|    n_updates            | 71090        |
|    policy_gradient_loss | -0.0293      |
|    value_loss           | 139          |
------------------------------------------
Epoch # 91
Eval num_timesteps=920000, episode_reward=7.06 +/- 3.98
Episode length: 9.00 +/- 2.76
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 7.06        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.018270656 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.9        |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.1        |
|    n_updates            | 71870       |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 37          |
-----------------------------------------
New best mean reward!
Epoch # 92
Eval num_timesteps=930000, episode_reward=8.21 +/- 4.71
Episode length: 9.20 +/- 2.93
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.2         |
|    mean_reward          | 8.21        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.021551121 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.72       |
|    explained_variance   | 0.219       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.6        |
|    n_updates            | 72650       |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 47.3        |
-----------------------------------------
New best mean reward!
Epoch # 93
Eval num_timesteps=940000, episode_reward=2.61 +/- 6.15
Episode length: 11.20 +/- 1.33
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.2        |
|    mean_reward          | 2.61        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 940000      |
| train/                  |             |
|    approx_kl            | 0.010418043 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.83       |
|    explained_variance   | 0.012       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 73430       |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 213         |
-----------------------------------------
Epoch # 94
Eval num_timesteps=950000, episode_reward=6.09 +/- 2.56
Episode length: 10.40 +/- 3.93
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10.4        |
|    mean_reward          | 6.09        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.022907685 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.62       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.2        |
|    n_updates            | 74210       |
|    policy_gradient_loss | -0.0358     |
|    value_loss           | 41.3        |
-----------------------------------------
Epoch # 95
Eval num_timesteps=960000, episode_reward=5.50 +/- 5.23
Episode length: 7.40 +/- 3.20
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 7.4         |
|    mean_reward          | 5.5         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.023882905 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.59       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.55        |
|    n_updates            | 74990       |
|    policy_gradient_loss | -0.041      |
|    value_loss           | 41          |
-----------------------------------------
Epoch # 96
Eval num_timesteps=970000, episode_reward=4.18 +/- 4.59
Episode length: 8.60 +/- 4.32
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 8.6        |
|    mean_reward          | 4.18       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 970000     |
| train/                  |            |
|    approx_kl            | 0.02205164 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.98      |
|    explained_variance   | -0.0766    |
|    learning_rate        | 0.0003     |
|    loss                 | 22.8       |
|    n_updates            | 75780      |
|    policy_gradient_loss | -0.0313    |
|    value_loss           | 114        |
----------------------------------------
Epoch # 97
Eval num_timesteps=980000, episode_reward=11.66 +/- 4.52
Episode length: 10.60 +/- 2.06
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 10.6       |
|    mean_reward          | 11.7       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 980000     |
| train/                  |            |
|    approx_kl            | 0.02681324 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.65      |
|    explained_variance   | 0.175      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.5       |
|    n_updates            | 76560      |
|    policy_gradient_loss | -0.044     |
|    value_loss           | 42.8       |
----------------------------------------
New best mean reward!
Epoch # 98
Eval num_timesteps=990000, episode_reward=7.97 +/- 3.08
Episode length: 9.40 +/- 2.65
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.4         |
|    mean_reward          | 7.97        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 990000      |
| train/                  |             |
|    approx_kl            | 0.038666956 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.0003      |
|    loss                 | 23.9        |
|    n_updates            | 77340       |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 36.5        |
-----------------------------------------
Epoch # 99
Eval num_timesteps=1000000, episode_reward=3.85 +/- 4.06
Episode length: 9.40 +/- 2.87
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.4         |
|    mean_reward          | 3.85        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.037189797 |
|    clip_fraction        | 0.37        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.105       |
|    learning_rate        | 0.0003      |
|    loss                 | 23          |
|    n_updates            | 78120       |
|    policy_gradient_loss | -0.04       |
|    value_loss           | 65.1        |
-----------------------------------------
Epoch # 100
Eval num_timesteps=1010000, episode_reward=13.21 +/- 6.85
Episode length: 13.60 +/- 3.14
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13.6       |
|    mean_reward          | 13.2       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1010000    |
| train/                  |            |
|    approx_kl            | 0.05288011 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.383      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.94       |
|    n_updates            | 78900      |
|    policy_gradient_loss | -0.0471    |
|    value_loss           | 37.9       |
----------------------------------------
New best mean reward!
Epoch # 101
Eval num_timesteps=1020000, episode_reward=14.24 +/- 9.43
Episode length: 11.20 +/- 5.23
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.2        |
|    mean_reward          | 14.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1020000     |
| train/                  |             |
|    approx_kl            | 0.033773795 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.7        |
|    n_updates            | 79680       |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 75.9        |
-----------------------------------------
New best mean reward!
Epoch # 102
Eval num_timesteps=1030000, episode_reward=10.99 +/- 9.93
Episode length: 11.60 +/- 1.96
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.6        |
|    mean_reward          | 11          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1030000     |
| train/                  |             |
|    approx_kl            | 0.041469745 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.92       |
|    explained_variance   | 0.408       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.7        |
|    n_updates            | 80460       |
|    policy_gradient_loss | -0.0574     |
|    value_loss           | 37.2        |
-----------------------------------------
Epoch # 103
Eval num_timesteps=1040000, episode_reward=11.47 +/- 2.69
Episode length: 11.80 +/- 1.47
Success rate: 0.00%
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 11.8     |
|    mean_reward          | 11.5     |
|    success_rate         | 0        |
| time/                   |          |
|    total_timesteps      | 1040000  |
| train/                  |          |
|    approx_kl            | 0.035866 |
|    clip_fraction        | 0.323    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.4     |
|    explained_variance   | 0.427    |
|    learning_rate        | 0.0003   |
|    loss                 | 24.3     |
|    n_updates            | 81240    |
|    policy_gradient_loss | -0.0326  |
|    value_loss           | 37.2     |
--------------------------------------
Epoch # 104
Eval num_timesteps=1050000, episode_reward=14.38 +/- 6.65
Episode length: 11.40 +/- 4.08
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 11.4       |
|    mean_reward          | 14.4       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1050000    |
| train/                  |            |
|    approx_kl            | 0.02410145 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.7       |
|    explained_variance   | 0.444      |
|    learning_rate        | 0.0003     |
|    loss                 | 17         |
|    n_updates            | 82030      |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 41         |
----------------------------------------
New best mean reward!
Epoch # 105
Eval num_timesteps=1060000, episode_reward=3.90 +/- 6.84
Episode length: 9.20 +/- 2.64
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.2         |
|    mean_reward          | 3.9         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1060000     |
| train/                  |             |
|    approx_kl            | 0.097055815 |
|    clip_fraction        | 0.527       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.3        |
|    n_updates            | 82810       |
|    policy_gradient_loss | -0.0418     |
|    value_loss           | 56.3        |
-----------------------------------------
Epoch # 106
Eval num_timesteps=1070000, episode_reward=10.56 +/- 6.66
Episode length: 11.60 +/- 1.85
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 11.6       |
|    mean_reward          | 10.6       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1070000    |
| train/                  |            |
|    approx_kl            | 0.03207233 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.33      |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.5       |
|    n_updates            | 83590      |
|    policy_gradient_loss | -0.0324    |
|    value_loss           | 60.9       |
----------------------------------------
Epoch # 107
Eval num_timesteps=1080000, episode_reward=9.11 +/- 5.58
Episode length: 10.80 +/- 3.49
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10.8        |
|    mean_reward          | 9.11        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1080000     |
| train/                  |             |
|    approx_kl            | 0.030637274 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.0477      |
|    learning_rate        | 0.0003      |
|    loss                 | 36.6        |
|    n_updates            | 84370       |
|    policy_gradient_loss | -0.0381     |
|    value_loss           | 89.5        |
-----------------------------------------
Epoch # 108
Eval num_timesteps=1090000, episode_reward=10.57 +/- 6.23
Episode length: 10.00 +/- 2.83
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10          |
|    mean_reward          | 10.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1090000     |
| train/                  |             |
|    approx_kl            | 0.021102615 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.4        |
|    n_updates            | 85150       |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 75          |
-----------------------------------------
Epoch # 109
Eval num_timesteps=1100000, episode_reward=3.55 +/- 3.72
Episode length: 10.60 +/- 4.18
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10.6        |
|    mean_reward          | 3.55        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.059119888 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.396       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.6        |
|    n_updates            | 85930       |
|    policy_gradient_loss | -0.0357     |
|    value_loss           | 46.8        |
-----------------------------------------
Epoch # 110
Eval num_timesteps=1110000, episode_reward=11.86 +/- 5.23
Episode length: 9.80 +/- 2.32
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.8         |
|    mean_reward          | 11.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1110000     |
| train/                  |             |
|    approx_kl            | 0.038950823 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    n_updates            | 86710       |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 35.9        |
-----------------------------------------
Epoch # 111
Eval num_timesteps=1120000, episode_reward=14.68 +/- 7.19
Episode length: 16.60 +/- 4.41
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | 14.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1120000     |
| train/                  |             |
|    approx_kl            | 0.015309218 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 22.3        |
|    n_updates            | 87490       |
|    policy_gradient_loss | -0.0373     |
|    value_loss           | 47.5        |
-----------------------------------------
New best mean reward!
Epoch # 112
Eval num_timesteps=1130000, episode_reward=16.62 +/- 8.17
Episode length: 11.80 +/- 3.31
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.8        |
|    mean_reward          | 16.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1130000     |
| train/                  |             |
|    approx_kl            | 0.045632217 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0003      |
|    loss                 | 18.3        |
|    n_updates            | 88280       |
|    policy_gradient_loss | -0.0388     |
|    value_loss           | 53.8        |
-----------------------------------------
New best mean reward!
Epoch # 113
Eval num_timesteps=1140000, episode_reward=12.72 +/- 7.90
Episode length: 12.00 +/- 5.55
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 12        |
|    mean_reward          | 12.7      |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 1140000   |
| train/                  |           |
|    approx_kl            | 0.0402653 |
|    clip_fraction        | 0.344     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.95     |
|    explained_variance   | 0.338     |
|    learning_rate        | 0.0003    |
|    loss                 | 31.3      |
|    n_updates            | 89060     |
|    policy_gradient_loss | -0.0475   |
|    value_loss           | 56.6      |
---------------------------------------
Epoch # 114
Eval num_timesteps=1150000, episode_reward=17.85 +/- 6.48
Episode length: 12.20 +/- 2.32
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 12.2        |
|    mean_reward          | 17.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1150000     |
| train/                  |             |
|    approx_kl            | 0.053999603 |
|    clip_fraction        | 0.395       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.2        |
|    explained_variance   | 0.0854      |
|    learning_rate        | 0.0003      |
|    loss                 | 23          |
|    n_updates            | 89840       |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 92          |
-----------------------------------------
New best mean reward!
Epoch # 115
Eval num_timesteps=1160000, episode_reward=18.40 +/- 13.00
Episode length: 14.00 +/- 5.66
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14          |
|    mean_reward          | 18.4        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1160000     |
| train/                  |             |
|    approx_kl            | 0.028522467 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.76       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.6        |
|    n_updates            | 90620       |
|    policy_gradient_loss | -0.04       |
|    value_loss           | 70.5        |
-----------------------------------------
New best mean reward!
Epoch # 116
Eval num_timesteps=1170000, episode_reward=12.70 +/- 3.87
Episode length: 10.40 +/- 1.62
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10.4        |
|    mean_reward          | 12.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1170000     |
| train/                  |             |
|    approx_kl            | 0.032760162 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.12        |
|    learning_rate        | 0.0003      |
|    loss                 | 56.8        |
|    n_updates            | 91400       |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 96.3        |
-----------------------------------------
Epoch # 117
Eval num_timesteps=1180000, episode_reward=26.44 +/- 9.86
Episode length: 15.00 +/- 3.29
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 26.4        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1180000     |
| train/                  |             |
|    approx_kl            | 0.027148787 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 31.2        |
|    n_updates            | 92180       |
|    policy_gradient_loss | -0.0446     |
|    value_loss           | 48.5        |
-----------------------------------------
New best mean reward!
Epoch # 118
Eval num_timesteps=1190000, episode_reward=22.43 +/- 10.08
Episode length: 14.00 +/- 4.60
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14         |
|    mean_reward          | 22.4       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1190000    |
| train/                  |            |
|    approx_kl            | 0.03205445 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.01      |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.0003     |
|    loss                 | 40.4       |
|    n_updates            | 92960      |
|    policy_gradient_loss | -0.0303    |
|    value_loss           | 58.4       |
----------------------------------------
Epoch # 119
Eval num_timesteps=1200000, episode_reward=21.89 +/- 10.56
Episode length: 13.40 +/- 2.50
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13.4       |
|    mean_reward          | 21.9       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1200000    |
| train/                  |            |
|    approx_kl            | 0.01651592 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.94      |
|    explained_variance   | 0.442      |
|    learning_rate        | 0.0003     |
|    loss                 | 35         |
|    n_updates            | 93740      |
|    policy_gradient_loss | -0.0354    |
|    value_loss           | 60.4       |
----------------------------------------
Epoch # 120
Eval num_timesteps=1210000, episode_reward=24.09 +/- 3.29
Episode length: 15.20 +/- 1.94
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | 24.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1210000     |
| train/                  |             |
|    approx_kl            | 0.064699754 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.87       |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.6        |
|    n_updates            | 94530       |
|    policy_gradient_loss | -0.0476     |
|    value_loss           | 51.3        |
-----------------------------------------
Epoch # 121
Eval num_timesteps=1220000, episode_reward=24.19 +/- 6.19
Episode length: 18.40 +/- 5.24
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.4        |
|    mean_reward          | 24.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1220000     |
| train/                  |             |
|    approx_kl            | 0.053370338 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.0003      |
|    loss                 | 40.5        |
|    n_updates            | 95310       |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 70.1        |
-----------------------------------------
Epoch # 122
Eval num_timesteps=1230000, episode_reward=32.23 +/- 6.23
Episode length: 17.20 +/- 2.32
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 32.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1230000     |
| train/                  |             |
|    approx_kl            | 0.049628004 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 30          |
|    n_updates            | 96090       |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 75.3        |
-----------------------------------------
New best mean reward!
Epoch # 123
Eval num_timesteps=1240000, episode_reward=36.32 +/- 13.68
Episode length: 16.80 +/- 2.48
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 36.3        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1240000     |
| train/                  |             |
|    approx_kl            | 0.023462608 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.0191      |
|    learning_rate        | 0.0003      |
|    loss                 | 41.1        |
|    n_updates            | 96870       |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 204         |
-----------------------------------------
New best mean reward!
Epoch # 124
Eval num_timesteps=1250000, episode_reward=26.87 +/- 12.29
Episode length: 15.20 +/- 3.31
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.2       |
|    mean_reward          | 26.9       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1250000    |
| train/                  |            |
|    approx_kl            | 0.10672592 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.14      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.1       |
|    n_updates            | 97650      |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 41         |
----------------------------------------
Epoch # 125
Eval num_timesteps=1260000, episode_reward=36.88 +/- 4.33
Episode length: 20.00 +/- 2.90
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20          |
|    mean_reward          | 36.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1260000     |
| train/                  |             |
|    approx_kl            | 0.062080637 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.465       |
|    learning_rate        | 0.0003      |
|    loss                 | 57.3        |
|    n_updates            | 98430       |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 116         |
-----------------------------------------
New best mean reward!
Epoch # 126
Eval num_timesteps=1270000, episode_reward=34.89 +/- 14.78
Episode length: 17.60 +/- 1.36
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 34.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1270000     |
| train/                  |             |
|    approx_kl            | 0.088518076 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 19          |
|    n_updates            | 99210       |
|    policy_gradient_loss | -0.0505     |
|    value_loss           | 43.8        |
-----------------------------------------
Epoch # 127
Eval num_timesteps=1280000, episode_reward=34.18 +/- 8.45
Episode length: 20.00 +/- 2.90
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20          |
|    mean_reward          | 34.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.060463686 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | 39.4        |
|    n_updates            | 99990       |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 84.6        |
-----------------------------------------
Epoch # 128
Eval num_timesteps=1290000, episode_reward=37.62 +/- 10.27
Episode length: 17.40 +/- 2.06
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.4       |
|    mean_reward          | 37.6       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1290000    |
| train/                  |            |
|    approx_kl            | 0.10683337 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.3       |
|    n_updates            | 100780     |
|    policy_gradient_loss | -0.0306    |
|    value_loss           | 50.9       |
----------------------------------------
New best mean reward!
Epoch # 129
Eval num_timesteps=1300000, episode_reward=46.47 +/- 14.13
Episode length: 19.20 +/- 1.60
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.2       |
|    mean_reward          | 46.5       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1300000    |
| train/                  |            |
|    approx_kl            | 0.04423804 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.85      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.77       |
|    n_updates            | 101560     |
|    policy_gradient_loss | -0.0415    |
|    value_loss           | 16.8       |
----------------------------------------
New best mean reward!
Epoch # 130
Eval num_timesteps=1310000, episode_reward=45.76 +/- 7.14
Episode length: 20.00 +/- 1.26
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20         |
|    mean_reward          | 45.8       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1310000    |
| train/                  |            |
|    approx_kl            | 0.06738512 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | 34.6       |
|    n_updates            | 102340     |
|    policy_gradient_loss | -0.0327    |
|    value_loss           | 56.8       |
----------------------------------------
Epoch # 131
Eval num_timesteps=1320000, episode_reward=44.68 +/- 16.25
Episode length: 19.60 +/- 1.62
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.6        |
|    mean_reward          | 44.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1320000     |
| train/                  |             |
|    approx_kl            | 0.041894272 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.5        |
|    n_updates            | 103120      |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 52.8        |
-----------------------------------------
Epoch # 132
Eval num_timesteps=1330000, episode_reward=47.07 +/- 7.95
Episode length: 20.20 +/- 1.94
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.2        |
|    mean_reward          | 47.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1330000     |
| train/                  |             |
|    approx_kl            | 0.047975235 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 18.5        |
|    n_updates            | 103900      |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 59.1        |
-----------------------------------------
New best mean reward!
Epoch # 133
Eval num_timesteps=1340000, episode_reward=17.06 +/- 16.82
Episode length: 9.60 +/- 5.24
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.6         |
|    mean_reward          | 17.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1340000     |
| train/                  |             |
|    approx_kl            | 0.058591943 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 13          |
|    n_updates            | 104680      |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 70.6        |
-----------------------------------------
Epoch # 134
Eval num_timesteps=1350000, episode_reward=55.60 +/- 13.64
Episode length: 21.00 +/- 3.22
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 55.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.040079642 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | 13.4        |
|    n_updates            | 105460      |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 25          |
-----------------------------------------
New best mean reward!
Epoch # 135
Eval num_timesteps=1360000, episode_reward=59.43 +/- 7.69
Episode length: 22.00 +/- 2.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22         |
|    mean_reward          | 59.4       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1360000    |
| train/                  |            |
|    approx_kl            | 0.04798799 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 15         |
|    n_updates            | 106240     |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 30.2       |
----------------------------------------
New best mean reward!
Epoch # 136
Eval num_timesteps=1370000, episode_reward=59.58 +/- 1.85
Episode length: 22.00 +/- 2.28
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22          |
|    mean_reward          | 59.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1370000     |
| train/                  |             |
|    approx_kl            | 0.024328746 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.2        |
|    n_updates            | 107030      |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 69.5        |
-----------------------------------------
New best mean reward!
Epoch # 137
Eval num_timesteps=1380000, episode_reward=42.65 +/- 14.62
Episode length: 19.00 +/- 3.63
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19         |
|    mean_reward          | 42.7       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1380000    |
| train/                  |            |
|    approx_kl            | 0.07750654 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | 55.3       |
|    n_updates            | 107810     |
|    policy_gradient_loss | -0.0316    |
|    value_loss           | 71.5       |
----------------------------------------
Epoch # 138
Eval num_timesteps=1390000, episode_reward=58.29 +/- 6.85
Episode length: 21.20 +/- 1.33
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.2       |
|    mean_reward          | 58.3       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1390000    |
| train/                  |            |
|    approx_kl            | 0.06436701 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | 26.7       |
|    n_updates            | 108590     |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 58.7       |
----------------------------------------
Epoch # 139
Eval num_timesteps=1400000, episode_reward=65.76 +/- 5.59
Episode length: 23.60 +/- 1.36
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23.6       |
|    mean_reward          | 65.8       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1400000    |
| train/                  |            |
|    approx_kl            | 0.09110062 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.57       |
|    learning_rate        | 0.0003     |
|    loss                 | 96.3       |
|    n_updates            | 109370     |
|    policy_gradient_loss | -0.0324    |
|    value_loss           | 152        |
----------------------------------------
New best mean reward!
Epoch # 140
Eval num_timesteps=1410000, episode_reward=50.61 +/- 10.78
Episode length: 22.00 +/- 3.29
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22         |
|    mean_reward          | 50.6       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1410000    |
| train/                  |            |
|    approx_kl            | 0.02993298 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | 34         |
|    n_updates            | 110150     |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 113        |
----------------------------------------
Epoch # 141
Eval num_timesteps=1420000, episode_reward=76.50 +/- 2.29
Episode length: 24.20 +/- 1.17
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 24.2      |
|    mean_reward          | 76.5      |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 1420000   |
| train/                  |           |
|    approx_kl            | 0.1449126 |
|    clip_fraction        | 0.196     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.05     |
|    explained_variance   | 0.879     |
|    learning_rate        | 0.0003    |
|    loss                 | 32.8      |
|    n_updates            | 110930    |
|    policy_gradient_loss | -0.0278   |
|    value_loss           | 57.2      |
---------------------------------------
New best mean reward!
Epoch # 142
Eval num_timesteps=1430000, episode_reward=63.27 +/- 13.75
Episode length: 22.40 +/- 3.20
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22.4       |
|    mean_reward          | 63.3       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1430000    |
| train/                  |            |
|    approx_kl            | 0.09604425 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 16         |
|    n_updates            | 111710     |
|    policy_gradient_loss | -0.0293    |
|    value_loss           | 130        |
----------------------------------------
Epoch # 143
Eval num_timesteps=1440000, episode_reward=80.91 +/- 2.23
Episode length: 24.60 +/- 1.02
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.6        |
|    mean_reward          | 80.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1440000     |
| train/                  |             |
|    approx_kl            | 0.037532397 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.3        |
|    n_updates            | 112490      |
|    policy_gradient_loss | -0.037      |
|    value_loss           | 42.8        |
-----------------------------------------
New best mean reward!
Epoch # 144
Eval num_timesteps=1450000, episode_reward=70.24 +/- 2.32
Episode length: 25.40 +/- 0.80
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.4        |
|    mean_reward          | 70.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1450000     |
| train/                  |             |
|    approx_kl            | 0.121189505 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.516       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.5        |
|    n_updates            | 113280      |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 206         |
-----------------------------------------
Epoch # 145
Eval num_timesteps=1460000, episode_reward=83.47 +/- 8.54
Episode length: 26.00 +/- 1.79
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26         |
|    mean_reward          | 83.5       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1460000    |
| train/                  |            |
|    approx_kl            | 0.06005419 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.908     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 26.9       |
|    n_updates            | 114060     |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 96.4       |
----------------------------------------
New best mean reward!
Epoch # 146
Eval num_timesteps=1470000, episode_reward=63.80 +/- 10.56
Episode length: 23.20 +/- 1.94
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.2        |
|    mean_reward          | 63.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1470000     |
| train/                  |             |
|    approx_kl            | 0.058449946 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.937      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | 57          |
|    n_updates            | 114840      |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 95.6        |
-----------------------------------------
Epoch # 147
Eval num_timesteps=1480000, episode_reward=79.18 +/- 7.01
Episode length: 26.00 +/- 1.79
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26          |
|    mean_reward          | 79.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1480000     |
| train/                  |             |
|    approx_kl            | 0.027917732 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | 12          |
|    n_updates            | 115620      |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 35.6        |
-----------------------------------------
Epoch # 148
Eval num_timesteps=1490000, episode_reward=92.77 +/- 4.35
Episode length: 26.60 +/- 0.80
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.6        |
|    mean_reward          | 92.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1490000     |
| train/                  |             |
|    approx_kl            | 0.019551411 |
|    clip_fraction        | 0.0727      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | 47.7        |
|    n_updates            | 116400      |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 76.1        |
-----------------------------------------
New best mean reward!
Epoch # 149
Eval num_timesteps=1500000, episode_reward=84.97 +/- 8.76
Episode length: 27.20 +/- 1.33
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.2        |
|    mean_reward          | 85          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1500000     |
| train/                  |             |
|    approx_kl            | 0.030204443 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 183         |
|    n_updates            | 117180      |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 352         |
-----------------------------------------
Epoch # 150
Eval num_timesteps=1510000, episode_reward=92.32 +/- 9.12
Episode length: 27.20 +/- 1.47
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.2        |
|    mean_reward          | 92.3        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1510000     |
| train/                  |             |
|    approx_kl            | 0.035739835 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.635      |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 51.4        |
|    n_updates            | 117960      |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 89.1        |
-----------------------------------------
Epoch # 151
Eval num_timesteps=1520000, episode_reward=98.82 +/- 4.04
Episode length: 27.40 +/- 1.02
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 27.4      |
|    mean_reward          | 98.8      |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 1520000   |
| train/                  |           |
|    approx_kl            | 0.0955941 |
|    clip_fraction        | 0.141     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.54     |
|    explained_variance   | 0.793     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.4       |
|    n_updates            | 118740    |
|    policy_gradient_loss | -0.00929  |
|    value_loss           | 127       |
---------------------------------------
New best mean reward!
Epoch # 152
Eval num_timesteps=1530000, episode_reward=93.81 +/- 13.01
Episode length: 27.20 +/- 2.40
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 27.2      |
|    mean_reward          | 93.8      |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 1530000   |
| train/                  |           |
|    approx_kl            | 0.0552334 |
|    clip_fraction        | 0.187     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.746    |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | 11.2      |
|    n_updates            | 119530    |
|    policy_gradient_loss | -0.0305   |
|    value_loss           | 27.7      |
---------------------------------------
Epoch # 153
Eval num_timesteps=1540000, episode_reward=99.26 +/- 4.15
Episode length: 27.80 +/- 0.75
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.8        |
|    mean_reward          | 99.3        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1540000     |
| train/                  |             |
|    approx_kl            | 0.046432324 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 94.1        |
|    n_updates            | 120310      |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 110         |
-----------------------------------------
New best mean reward!
Epoch # 154
Eval num_timesteps=1550000, episode_reward=81.98 +/- 20.96
Episode length: 25.20 +/- 3.82
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.2       |
|    mean_reward          | 82         |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1550000    |
| train/                  |            |
|    approx_kl            | 0.17620221 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.747     |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.0003     |
|    loss                 | 83.3       |
|    n_updates            | 121090     |
|    policy_gradient_loss | 0.00171    |
|    value_loss           | 279        |
----------------------------------------
Epoch # 155
Eval num_timesteps=1560000, episode_reward=63.06 +/- 34.49
Episode length: 21.40 +/- 4.80
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.4       |
|    mean_reward          | 63.1       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1560000    |
| train/                  |            |
|    approx_kl            | 0.07875915 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.869     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | 60.2       |
|    n_updates            | 121870     |
|    policy_gradient_loss | -0.0269    |
|    value_loss           | 279        |
----------------------------------------
Epoch # 156
Eval num_timesteps=1570000, episode_reward=99.11 +/- 7.44
Episode length: 30.00 +/- 2.37
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 99.1      |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 1570000   |
| train/                  |           |
|    approx_kl            | 0.0417616 |
|    clip_fraction        | 0.0766    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.465    |
|    explained_variance   | 0.915     |
|    learning_rate        | 0.0003    |
|    loss                 | 27.6      |
|    n_updates            | 122650    |
|    policy_gradient_loss | -0.0168   |
|    value_loss           | 71.8      |
---------------------------------------
Epoch # 157
Eval num_timesteps=1580000, episode_reward=110.89 +/- 1.18
Episode length: 29.00 +/- 0.63
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 111         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1580000     |
| train/                  |             |
|    approx_kl            | 0.029725133 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.1        |
|    n_updates            | 123430      |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 68.2        |
-----------------------------------------
New best mean reward!
Epoch # 158
Eval num_timesteps=1590000, episode_reward=108.69 +/- 7.26
Episode length: 28.00 +/- 0.63
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28         |
|    mean_reward          | 109        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1590000    |
| train/                  |            |
|    approx_kl            | 0.04571931 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.589     |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.2       |
|    n_updates            | 124210     |
|    policy_gradient_loss | -0.00296   |
|    value_loss           | 59.6       |
----------------------------------------
Epoch # 159
Eval num_timesteps=1600000, episode_reward=109.08 +/- 23.04
Episode length: 27.60 +/- 2.80
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.6        |
|    mean_reward          | 109         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.014119534 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.5        |
|    n_updates            | 124990      |
|    policy_gradient_loss | -0.0082     |
|    value_loss           | 253         |
-----------------------------------------
Epoch # 160
Eval num_timesteps=1610000, episode_reward=107.82 +/- 7.44
Episode length: 29.40 +/- 0.49
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.4       |
|    mean_reward          | 108        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1610000    |
| train/                  |            |
|    approx_kl            | 0.10435502 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | 53.6       |
|    n_updates            | 125780     |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 114        |
----------------------------------------
Epoch # 161
Eval num_timesteps=1620000, episode_reward=83.07 +/- 15.81
Episode length: 26.00 +/- 2.10
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26         |
|    mean_reward          | 83.1       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1620000    |
| train/                  |            |
|    approx_kl            | 0.15962481 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 7.57       |
|    n_updates            | 126560     |
|    policy_gradient_loss | -0.0239    |
|    value_loss           | 112        |
----------------------------------------
Epoch # 162
Eval num_timesteps=1630000, episode_reward=89.14 +/- 12.28
Episode length: 28.00 +/- 3.03
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28          |
|    mean_reward          | 89.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1630000     |
| train/                  |             |
|    approx_kl            | 0.021785218 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 42.9        |
|    n_updates            | 127340      |
|    policy_gradient_loss | -0.00463    |
|    value_loss           | 382         |
-----------------------------------------
Epoch # 163
Eval num_timesteps=1640000, episode_reward=103.65 +/- 7.78
Episode length: 27.80 +/- 0.40
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 27.8      |
|    mean_reward          | 104       |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 1640000   |
| train/                  |           |
|    approx_kl            | 0.3553739 |
|    clip_fraction        | 0.136     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.529    |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.3       |
|    n_updates            | 128120    |
|    policy_gradient_loss | -0.0297   |
|    value_loss           | 9.93      |
---------------------------------------
Epoch # 164
Eval num_timesteps=1650000, episode_reward=107.42 +/- 2.99
Episode length: 31.00 +/- 1.79
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31          |
|    mean_reward          | 107         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.025691874 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 79          |
|    n_updates            | 128900      |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 263         |
-----------------------------------------
Epoch # 165
Eval num_timesteps=1660000, episode_reward=87.18 +/- 8.37
Episode length: 25.80 +/- 1.17
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.8       |
|    mean_reward          | 87.2       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1660000    |
| train/                  |            |
|    approx_kl            | 0.15663524 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.461     |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | 26.8       |
|    n_updates            | 129680     |
|    policy_gradient_loss | -0.0235    |
|    value_loss           | 44         |
----------------------------------------
Epoch # 166
Eval num_timesteps=1670000, episode_reward=57.68 +/- 36.37
Episode length: 18.80 +/- 7.68
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.8       |
|    mean_reward          | 57.7       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1670000    |
| train/                  |            |
|    approx_kl            | 0.15573761 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.468     |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.9       |
|    n_updates            | 130460     |
|    policy_gradient_loss | -0.0212    |
|    value_loss           | 40.9       |
----------------------------------------
Epoch # 167
Eval num_timesteps=1680000, episode_reward=115.78 +/- 6.85
Episode length: 30.40 +/- 1.36
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.4       |
|    mean_reward          | 116        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1680000    |
| train/                  |            |
|    approx_kl            | 0.03154191 |
|    clip_fraction        | 0.05       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.303     |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 111        |
|    n_updates            | 131240     |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 141        |
----------------------------------------
New best mean reward!
Epoch # 168
Eval num_timesteps=1690000, episode_reward=87.66 +/- 27.06
Episode length: 24.80 +/- 3.43
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.8        |
|    mean_reward          | 87.7        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1690000     |
| train/                  |             |
|    approx_kl            | 0.018771108 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 83.5        |
|    n_updates            | 132030      |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 120         |
-----------------------------------------
Epoch # 169
Eval num_timesteps=1700000, episode_reward=60.90 +/- 28.21
Episode length: 23.60 +/- 5.39
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.6        |
|    mean_reward          | 60.9        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.030461796 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.77       |
|    explained_variance   | 0.391       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.3        |
|    n_updates            | 132810      |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 251         |
-----------------------------------------
Epoch # 170
Eval num_timesteps=1710000, episode_reward=100.37 +/- 17.82
Episode length: 28.20 +/- 3.82
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.2        |
|    mean_reward          | 100         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1710000     |
| train/                  |             |
|    approx_kl            | 0.042170763 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.2        |
|    n_updates            | 133590      |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 57          |
-----------------------------------------
Epoch # 171
Eval num_timesteps=1720000, episode_reward=106.73 +/- 7.66
Episode length: 29.40 +/- 0.49
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.4       |
|    mean_reward          | 107        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1720000    |
| train/                  |            |
|    approx_kl            | 0.15006886 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.513     |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.54       |
|    n_updates            | 134370     |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 17.2       |
----------------------------------------
Epoch # 172
Eval num_timesteps=1730000, episode_reward=104.40 +/- 19.11
Episode length: 27.60 +/- 2.15
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 27.6      |
|    mean_reward          | 104       |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 1730000   |
| train/                  |           |
|    approx_kl            | 0.0990182 |
|    clip_fraction        | 0.153     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.433    |
|    explained_variance   | 0.9       |
|    learning_rate        | 0.0003    |
|    loss                 | 17.9      |
|    n_updates            | 135150    |
|    policy_gradient_loss | -0.0298   |
|    value_loss           | 72.9      |
---------------------------------------
Epoch # 173
Eval num_timesteps=1740000, episode_reward=110.57 +/- 21.78
Episode length: 28.80 +/- 2.56
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 28.8         |
|    mean_reward          | 111          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 1740000      |
| train/                  |              |
|    approx_kl            | 0.0093303155 |
|    clip_fraction        | 0.0648       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.47        |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.0003       |
|    loss                 | 45.8         |
|    n_updates            | 135930       |
|    policy_gradient_loss | -0.0166      |
|    value_loss           | 150          |
------------------------------------------
Epoch # 174
Eval num_timesteps=1750000, episode_reward=118.93 +/- 3.35
Episode length: 30.00 +/- 2.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 119        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1750000    |
| train/                  |            |
|    approx_kl            | 0.07586691 |
|    clip_fraction        | 0.0656     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.44      |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0003     |
|    loss                 | 175        |
|    n_updates            | 136710     |
|    policy_gradient_loss | -0.024     |
|    value_loss           | 221        |
----------------------------------------
New best mean reward!
Epoch # 175
Eval num_timesteps=1760000, episode_reward=120.24 +/- 0.73
Episode length: 29.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 120         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1760000     |
| train/                  |             |
|    approx_kl            | 0.025791245 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.365      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.74        |
|    n_updates            | 137490      |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 7.3         |
-----------------------------------------
New best mean reward!
Epoch # 176
Eval num_timesteps=1770000, episode_reward=97.35 +/- 11.27
Episode length: 27.60 +/- 1.20
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.6        |
|    mean_reward          | 97.3        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1770000     |
| train/                  |             |
|    approx_kl            | 0.028476939 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 139         |
|    n_updates            | 138280      |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 164         |
-----------------------------------------
Epoch # 177
Eval num_timesteps=1780000, episode_reward=36.81 +/- 37.25
Episode length: 15.20 +/- 9.09
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | 36.8        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1780000     |
| train/                  |             |
|    approx_kl            | 0.021045785 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.0003      |
|    loss                 | 81          |
|    n_updates            | 139060      |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 185         |
-----------------------------------------
Epoch # 178
Eval num_timesteps=1790000, episode_reward=58.19 +/- 28.80
Episode length: 21.20 +/- 6.43
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.2       |
|    mean_reward          | 58.2       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1790000    |
| train/                  |            |
|    approx_kl            | 0.14734355 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.453     |
|    explained_variance   | 0.475      |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 139840     |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 290        |
----------------------------------------
Epoch # 179
Eval num_timesteps=1800000, episode_reward=107.30 +/- 9.96
Episode length: 29.40 +/- 1.96
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.4       |
|    mean_reward          | 107        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1800000    |
| train/                  |            |
|    approx_kl            | 0.09663288 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.511     |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.0003     |
|    loss                 | 84.9       |
|    n_updates            | 140620     |
|    policy_gradient_loss | -0.0232    |
|    value_loss           | 272        |
----------------------------------------
Eval num_timesteps=1810000, episode_reward=120.24 +/- 0.73
Episode length: 29.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 120         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1810000     |
| train/                  |             |
|    approx_kl            | 0.031146932 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.372      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 141400      |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 355         |
-----------------------------------------
Epoch # 180
Eval num_timesteps=1820000, episode_reward=103.87 +/- 2.84
Episode length: 28.80 +/- 1.47
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.8        |
|    mean_reward          | 104         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1820000     |
| train/                  |             |
|    approx_kl            | 0.016069174 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.49       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | 16.6        |
|    n_updates            | 142180      |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 73.7        |
-----------------------------------------
Epoch # 181
Eval num_timesteps=1830000, episode_reward=120.24 +/- 0.73
Episode length: 29.20 +/- 0.40
Success rate: 0.00%
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 29.2     |
|    mean_reward          | 120      |
|    success_rate         | 0        |
| time/                   |          |
|    total_timesteps      | 1830000  |
| train/                  |          |
|    approx_kl            | 0.045402 |
|    clip_fraction        | 0.0594   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.254   |
|    explained_variance   | 0.987    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.23     |
|    n_updates            | 142960   |
|    policy_gradient_loss | -0.0167  |
|    value_loss           | 13.8     |
--------------------------------------
Epoch # 182
Eval num_timesteps=1840000, episode_reward=93.54 +/- 11.40
Episode length: 26.40 +/- 1.36
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.4        |
|    mean_reward          | 93.5        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1840000     |
| train/                  |             |
|    approx_kl            | 0.044988953 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0003      |
|    loss                 | 145         |
|    n_updates            | 143740      |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 320         |
-----------------------------------------
Epoch # 183
Eval num_timesteps=1850000, episode_reward=100.87 +/- 6.74
Episode length: 27.20 +/- 0.75
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.2        |
|    mean_reward          | 101         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.007139889 |
|    clip_fraction        | 0.0484      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.414      |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 30.9        |
|    n_updates            | 144530      |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 90          |
-----------------------------------------
Epoch # 184
Eval num_timesteps=1860000, episode_reward=91.96 +/- 9.54
Episode length: 27.40 +/- 2.06
Success rate: 0.00%
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 27.4      |
|    mean_reward          | 92        |
|    success_rate         | 0         |
| time/                   |           |
|    total_timesteps      | 1860000   |
| train/                  |           |
|    approx_kl            | 0.0292251 |
|    clip_fraction        | 0.108     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.535    |
|    explained_variance   | 0.794     |
|    learning_rate        | 0.0003    |
|    loss                 | 31.2      |
|    n_updates            | 145310    |
|    policy_gradient_loss | -0.027    |
|    value_loss           | 126       |
---------------------------------------
Epoch # 185
Eval num_timesteps=1870000, episode_reward=62.59 +/- 28.88
Episode length: 21.80 +/- 3.97
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.8        |
|    mean_reward          | 62.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1870000     |
| train/                  |             |
|    approx_kl            | 0.028120562 |
|    clip_fraction        | 0.0734      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0003      |
|    loss                 | 249         |
|    n_updates            | 146090      |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 465         |
-----------------------------------------
Epoch # 186
Eval num_timesteps=1880000, episode_reward=101.60 +/- 9.14
Episode length: 28.20 +/- 1.47
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.2       |
|    mean_reward          | 102        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1880000    |
| train/                  |            |
|    approx_kl            | 0.06810217 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.531     |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.0003     |
|    loss                 | 49.1       |
|    n_updates            | 146870     |
|    policy_gradient_loss | -0.0273    |
|    value_loss           | 104        |
----------------------------------------
Epoch # 187
Eval num_timesteps=1890000, episode_reward=53.50 +/- 29.17
Episode length: 18.80 +/- 7.52
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.8       |
|    mean_reward          | 53.5       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1890000    |
| train/                  |            |
|    approx_kl            | 0.14269504 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.575     |
|    explained_variance   | 0.548      |
|    learning_rate        | 0.0003     |
|    loss                 | 80.8       |
|    n_updates            | 147650     |
|    policy_gradient_loss | -0.0317    |
|    value_loss           | 160        |
----------------------------------------
Epoch # 188
Eval num_timesteps=1900000, episode_reward=104.81 +/- 0.79
Episode length: 27.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.2        |
|    mean_reward          | 105         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1900000     |
| train/                  |             |
|    approx_kl            | 0.006418431 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.4        |
|    n_updates            | 148430      |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 25.8        |
-----------------------------------------
Epoch # 189
Eval num_timesteps=1910000, episode_reward=95.38 +/- 10.86
Episode length: 26.40 +/- 1.36
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26.4       |
|    mean_reward          | 95.4       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1910000    |
| train/                  |            |
|    approx_kl            | 0.03184235 |
|    clip_fraction        | 0.0523     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.298     |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 44.2       |
|    n_updates            | 149210     |
|    policy_gradient_loss | -0.0164    |
|    value_loss           | 66.4       |
----------------------------------------
Epoch # 190
Eval num_timesteps=1920000, episode_reward=91.57 +/- 5.49
Episode length: 27.00 +/- 1.26
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27          |
|    mean_reward          | 91.6        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1920000     |
| train/                  |             |
|    approx_kl            | 0.022929365 |
|    clip_fraction        | 0.0617      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.357      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | 27.1        |
|    n_updates            | 149990      |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 54.3        |
-----------------------------------------
Epoch # 191
Eval num_timesteps=1930000, episode_reward=92.54 +/- 9.85
Episode length: 26.40 +/- 1.20
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.4        |
|    mean_reward          | 92.5        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1930000     |
| train/                  |             |
|    approx_kl            | 0.019523442 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 150780      |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 269         |
-----------------------------------------
Epoch # 192
Eval num_timesteps=1940000, episode_reward=101.77 +/- 2.47
Episode length: 28.80 +/- 1.33
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.8       |
|    mean_reward          | 102        |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1940000    |
| train/                  |            |
|    approx_kl            | 0.09566371 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.435     |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 28.9       |
|    n_updates            | 151560     |
|    policy_gradient_loss | -0.0402    |
|    value_loss           | 94.8       |
----------------------------------------
Epoch # 193
Eval num_timesteps=1950000, episode_reward=96.95 +/- 7.73
Episode length: 27.00 +/- 1.10
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27         |
|    mean_reward          | 97         |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 1950000    |
| train/                  |            |
|    approx_kl            | 0.06931914 |
|    clip_fraction        | 0.0617     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.458     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | 91.2       |
|    n_updates            | 152340     |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 150        |
----------------------------------------
Epoch # 194
Eval num_timesteps=1960000, episode_reward=120.24 +/- 0.73
Episode length: 29.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 120         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1960000     |
| train/                  |             |
|    approx_kl            | 0.017987914 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.273      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.5        |
|    n_updates            | 153120      |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 29          |
-----------------------------------------
Epoch # 195
Eval num_timesteps=1970000, episode_reward=112.39 +/- 0.76
Episode length: 28.20 +/- 0.40
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.2        |
|    mean_reward          | 112         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1970000     |
| train/                  |             |
|    approx_kl            | 0.014034152 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.58        |
|    n_updates            | 153900      |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 10.6        |
-----------------------------------------
Epoch # 196
Eval num_timesteps=1980000, episode_reward=120.68 +/- 1.28
Episode length: 30.20 +/- 1.94
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.2        |
|    mean_reward          | 121         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1980000     |
| train/                  |             |
|    approx_kl            | 0.022233134 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 154680      |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 218         |
-----------------------------------------
New best mean reward!
Epoch # 197
Eval num_timesteps=1990000, episode_reward=116.02 +/- 7.51
Episode length: 29.20 +/- 0.98
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 116         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 1990000     |
| train/                  |             |
|    approx_kl            | 0.005387755 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.234      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 176         |
|    n_updates            | 155460      |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 260         |
-----------------------------------------
Epoch # 198
Eval num_timesteps=2000000, episode_reward=95.66 +/- 29.80
Episode length: 25.60 +/- 4.59
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.6       |
|    mean_reward          | 95.7       |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 2000000    |
| train/                  |            |
|    approx_kl            | 0.03642443 |
|    clip_fraction        | 0.0578     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.215     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.0003     |
|    loss                 | 342        |
|    n_updates            | 156240     |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 545        |
----------------------------------------
Epoch # 199
Eval num_timesteps=2010000, episode_reward=114.62 +/- 10.28
Episode length: 29.00 +/- 1.26
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 115         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 2010000     |
| train/                  |             |
|    approx_kl            | 0.020277355 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.8        |
|    n_updates            | 157030      |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 42.5        |
-----------------------------------------
Epoch # 200
Eval num_timesteps=2020000, episode_reward=117.72 +/- 9.86
Episode length: 30.00 +/- 0.63
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 118         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 2020000     |
| train/                  |             |
|    approx_kl            | 0.042108495 |
|    clip_fraction        | 0.0797      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.299      |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.7        |
|    n_updates            | 157810      |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 56.4        |
-----------------------------------------
Occupancy Ratio []
mean_reward:-4.04
mean_episode_len:1.89
defaultdict(<class 'int'>, {'BIN_OVERFLOW': 100})
Average Occupancy ratio: nan
Average time per input: 0.014808814525604248